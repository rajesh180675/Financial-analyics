# elite_financial_analytics_platform_v3_integrated.py
# Enterprise-Grade Financial Analytics Platform - Integrated Version with New Features, Optimizations, and UI Enhancements

# --- 1. Core Imports and Setup ---
import asyncio
import concurrent.futures
import functools
import hashlib
import io
import json
import logging
import os
import pickle
import re
import sys
import threading
import time
import traceback
import warnings
from abc import ABC, abstractmethod
from collections import defaultdict, deque
from contextlib import contextmanager
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta
from enum import Enum, auto
from pathlib import Path
from typing import (
    Any, Dict, List, Optional, Tuple, Union, Set, TypeVar, Generic, 
    Callable, Protocol, Type, cast, overload
)
from weakref import WeakValueDictionary

import numpy as np
import pandas as pd
from scipy import stats
from scipy.optimize import minimize

import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

from sklearn.linear_model import LinearRegression, Ridge
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.metrics.pairwise import cosine_similarity

import streamlit as st
from streamlit.runtime.uploaded_file_manager import UploadedFile

import bleach
from fuzzywuzzy import fuzz

# Try to import sentence transformers
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMER_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMER_AVAILABLE = False

# Configure logging with rotation
from logging.handlers import RotatingFileHandler

# Set up warnings
warnings.filterwarnings('ignore')

# --- Import Core Components (with fallback) ---
try:
    from financial_analytics_core import (
        ChartGenerator as CoreChartGenerator,
        FinancialRatioCalculator as CoreRatioCalculator,
        PenmanNissimAnalyzer as CorePenmanNissim,
        IndustryBenchmarks as CoreIndustryBenchmarks,
        DataProcessor as CoreDataProcessor,
        DataQualityMetrics,
        parse_html_content,
        parse_csv_content,
        process_and_merge_dataframes,
        REQUIRED_METRICS as CORE_REQUIRED_METRICS,
        YEAR_REGEX as CORE_YEAR_REGEX,
        MAX_FILE_SIZE_MB as CORE_MAX_FILE_SIZE,
        ALLOWED_FILE_TYPES as CORE_ALLOWED_TYPES,
        EPS
    )
    CORE_COMPONENTS_AVAILABLE = True
except ImportError:
    CORE_COMPONENTS_AVAILABLE = False
    # Define fallbacks
    CORE_REQUIRED_METRICS = {}
    CORE_YEAR_REGEX = re.compile(r'(20\d{2}|19\d{2}|FY\s?20\d{2}|FY\s?19\d{2})')
    CORE_MAX_FILE_SIZE = 10
    CORE_ALLOWED_TYPES = ['csv', 'html', 'htm', 'xls', 'xlsx']

# --- 2. Advanced Logging Configuration ---
class LoggerFactory:
    """Factory for creating configured loggers with context"""
    
    _loggers = {}
    _lock = threading.Lock()
    
    @classmethod
    def get_logger(cls, name: str, level: int = logging.INFO) -> logging.Logger:
        """Get or create a logger with proper configuration"""
        with cls._lock:
            if name not in cls._loggers:
                logger = logging.getLogger(name)
                logger.setLevel(level)
                
                # Console handler
                console_handler = logging.StreamHandler()
                console_handler.setLevel(level)
                
                # File handler with rotation
                log_dir = Path("logs")
                log_dir.mkdir(exist_ok=True)
                file_handler = RotatingFileHandler(
                    log_dir / f"{name}.log",
                    maxBytes=10485760,  # 10MB
                    backupCount=5
                )
                file_handler.setLevel(level)
                
                # Formatter
                formatter = logging.Formatter(
                    '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'
                )
                console_handler.setFormatter(formatter)
                file_handler.setFormatter(formatter)
                
                logger.addHandler(console_handler)
                logger.addHandler(file_handler)
                
                cls._loggers[name] = logger
            
            return cls._loggers[name]

# --- 3. Advanced Error Handling ---
class ErrorContext:
    """Context manager for error handling with recovery"""
    
    def __init__(self, operation: str, logger: logging.Logger, 
                 fallback: Optional[Callable] = None,
                 max_retries: int = 3):
        self.operation = operation
        self.logger = logger
        self.fallback = fallback
        self.max_retries = max_retries
        self.attempts = 0
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if exc_type is not None:
            self.attempts += 1
            self.logger.error(
                f"Error in {self.operation} (attempt {self.attempts}/{self.max_retries}): "
                f"{exc_type.__name__}: {exc_val}"
            )
            
            if self.attempts < self.max_retries:
                self.logger.info(f"Retrying {self.operation}...")
                return True  # Suppress exception for retry
            
            if self.fallback:
                self.logger.info(f"Executing fallback for {self.operation}")
                try:
                    self.fallback()
                except Exception as fallback_error:
                    self.logger.error(f"Fallback failed: {fallback_error}")
            
            # Log full traceback for debugging
            self.logger.debug(f"Full traceback:\n{''.join(traceback.format_tb(exc_tb))}")
            
        return False

# --- 4. Advanced Configuration Management ---
class ConfigurationError(Exception):
    """Custom exception for configuration errors"""
    pass

class Configuration:
    """Centralized configuration with validation and type safety"""
    
    # Type definitions
    class DisplayMode(Enum):
        FULL = auto()
        LITE = auto()
        MINIMAL = auto()
    
    class NumberFormat(Enum):
        INDIAN = auto()
        INTERNATIONAL = auto()
    
    # Default configurations
    DEFAULTS = {
        'app': {
            'version': '3.0.0',
            'name': 'Elite Financial Analytics Platform',
            'debug': False,
            'display_mode': DisplayMode.LITE,
            'max_file_size_mb': 10,
            'allowed_file_types': ['csv', 'html', 'htm', 'xls', 'xlsx'],
            'cache_ttl_seconds': 3600,
            'max_cache_size_mb': 100,
        },
        'processing': {
            'max_workers': 4,
            'chunk_size': 1000,
            'timeout_seconds': 30,
            'memory_limit_mb': 512,
            'enable_parallel': True,
        },
        'analysis': {
            'confidence_threshold': 0.6,
            'outlier_std_threshold': 3,
            'min_data_points': 3,
            'interpolation_method': 'linear',
            'number_format': NumberFormat.INDIAN,
        },
        'ai': {
            'enabled': True,
            'model_name': 'all-MiniLM-L6-v2',
            'batch_size': 32,
            'max_sequence_length': 512,
            'similarity_threshold': 0.6,
        },
        'ui': {
            'theme': 'light',
            'animations': True,
            'auto_save': True,
            'auto_save_interval': 60,
        }
    }
    
    def __init__(self, custom_config: Optional[Dict[str, Any]] = None):
        self._config = self._deep_merge(self.DEFAULTS.copy(), custom_config or {})
        self._validate_config()
        self._logger = LoggerFactory.get_logger('Configuration')
    
    def _deep_merge(self, base: Dict, override: Dict) -> Dict:
        """Deep merge two dictionaries"""
        for key, value in override.items():
            if key in base and isinstance(base[key], dict) and isinstance(value, dict):
                base[key] = self._deep_merge(base[key], value)
            else:
                base[key] = value
        return base
    
    def _validate_config(self):
        """Validate configuration values"""
        # App validation
        if self._config['app']['max_file_size_mb'] <= 0:
            raise ConfigurationError("max_file_size_mb must be positive")
        
        if not self._config['app']['allowed_file_types']:
            raise ConfigurationError("allowed_file_types cannot be empty")
        
        # Processing validation
        if self._config['processing']['max_workers'] <= 0:
            raise ConfigurationError("max_workers must be positive")
        
        if self._config['processing']['timeout_seconds'] <= 0:
            raise ConfigurationError("timeout_seconds must be positive")
        
        # Analysis validation
        if not 0 < self._config['analysis']['confidence_threshold'] <= 1:
            raise ConfigurationError("confidence_threshold must be between 0 and 1")
    
    def get(self, path: str, default: Any = None) -> Any:
        """Get configuration value by dot-separated path"""
        try:
            value = self._config
            for key in path.split('.'):
                value = value[key]
            return value
        except (KeyError, TypeError):
            self._logger.warning(f"Configuration key '{path}' not found, using default: {default}")
            return default
    
    def set(self, path: str, value: Any):
        """Set configuration value by dot-separated path"""
        keys = path.split('.')
        target = self._config
        
        for key in keys[:-1]:
            if key not in target:
                target[key] = {}
            target = target[key]
        
        target[keys[-1]] = value
        self._logger.info(f"Configuration updated: {path} = {value}")
    
    @contextmanager
    def override(self, **overrides):
        """Temporarily override configuration values"""
        original = {}
        
        try:
            # Save original values and apply overrides
            for path, value in overrides.items():
                original[path] = self.get(path)
                self.set(path, value)
            
            yield self
            
        finally:
            # Restore original values
            for path, value in original.items():
                self.set(path, value)

# --- 5. Advanced Caching System ---
class CacheEntry:
    """Cache entry with metadata"""
    
    def __init__(self, value: Any, ttl: Optional[int] = None):
        self.value = value
        self.created_at = time.time()
        self.ttl = ttl
        self.access_count = 0
        self.last_accessed = time.time()
    
    def is_expired(self) -> bool:
        """Check if entry is expired"""
        if self.ttl is None:
            return False
        return time.time() - self.created_at > self.ttl
    
    def access(self) -> Any:
        """Access the value and update metadata"""
        self.access_count += 1
        self.last_accessed = time.time()
        return self.value

class AdvancedCache:
    """Thread-safe cache with TTL, size limits, and statistics"""
    
    def __init__(self, max_size_mb: int = 100, default_ttl: int = 3600):
        self._cache: Dict[str, CacheEntry] = {}
        self._lock = threading.RLock()
        self._max_size_bytes = max_size_mb * 1024 * 1024
        self._default_ttl = default_ttl
        self._stats = defaultdict(int)
        self._logger = LoggerFactory.get_logger('Cache')
    
    def _estimate_size(self, obj: Any) -> int:
        """Estimate object size in bytes"""
        try:
            return len(pickle.dumps(obj))
        except Exception:
            return 1024  # Default estimate
    
    def _evict_if_needed(self):
        """Evict entries if cache is too large"""
        current_size = sum(self._estimate_size(entry.value) for entry in self._cache.values())
        
        if current_size > self._max_size_bytes:
            # Evict least recently used entries
            entries = sorted(
                self._cache.items(),
                key=lambda x: x[1].last_accessed
            )
            
            while current_size > self._max_size_bytes * 0.8 and entries:
                key, entry = entries.pop(0)
                current_size -= self._estimate_size(entry.value)
                del self._cache[key]
                self._stats['evictions'] += 1
                self._logger.debug(f"Evicted cache entry: {key}")
    
    def get(self, key: str) -> Optional[Any]:
        """Get value from cache"""
        with self._lock:
            self._stats['get_calls'] += 1
            
            if key in self._cache:
                entry = self._cache[key]
                
                if entry.is_expired():
                    del self._cache[key]
                    self._stats['expired'] += 1
                    return None
                
                self._stats['hits'] += 1
                return entry.access()
            
            self._stats['misses'] += 1
            return None
    
    def set(self, key: str, value: Any, ttl: Optional[int] = None):
        """Set value in cache"""
        with self._lock:
            self._stats['set_calls'] += 1
            
            # Create entry
            entry = CacheEntry(value, ttl or self._default_ttl)
            self._cache[key] = entry
            
            # Evict if needed
            self._evict_if_needed()
    
    def delete(self, key: str) -> bool:
        """Delete entry from cache"""
        with self._lock:
            if key in self._cache:
                del self._cache[key]
                self._stats['deletes'] += 1
                return True
            return False
    
    def clear(self):
        """Clear all cache entries"""
        with self._lock:
            self._cache.clear()
            self._stats['clears'] += 1
    
    def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics"""
        with self._lock:
            total_calls = self._stats['get_calls']
            hit_rate = (self._stats['hits'] / total_calls * 100) if total_calls > 0 else 0
            
            return {
                'entries': len(self._cache),
                'size_bytes': sum(self._estimate_size(e.value) for e in self._cache.values()),
                'hit_rate': hit_rate,
                **self._stats
            }

# --- 6. Resource Management ---
class ResourceManager:
    """Manage computational resources and prevent overload"""
    
    def __init__(self, config: Configuration):
        self.config = config
        self._semaphore = threading.Semaphore(config.get('processing.max_workers', 4))
        self._memory_limit = config.get('processing.memory_limit_mb', 512) * 1024 * 1024
        self._active_tasks = set()
        self._lock = threading.Lock()
        self._logger = LoggerFactory.get_logger('ResourceManager')
    
    @contextmanager
    def acquire_worker(self, task_name: str):
        """Acquire a worker slot for processing"""
        acquired = False
        start_time = time.time()
        
        try:
            # Try to acquire with timeout
            acquired = self._semaphore.acquire(
                timeout=self.config.get('processing.timeout_seconds', 30)
            )
            
            if not acquired:
                raise TimeoutError(f"Failed to acquire worker for {task_name}")
            
            with self._lock:
                self._active_tasks.add(task_name)
            
            self._logger.debug(f"Acquired worker for {task_name}")
            yield
            
        finally:
            if acquired:
                self._semaphore.release()
                
                with self._lock:
                    self._active_tasks.discard(task_name)
                
                elapsed = time.time() - start_time
                self._logger.debug(f"Released worker for {task_name} after {elapsed:.2f}s")
    
    def check_memory_available(self, estimated_size: int) -> bool:
        """Check if enough memory is available"""
        try:
            import psutil
            available = psutil.virtual_memory().available
            return available > estimated_size + self._memory_limit
        except ImportError:
            # Fallback if psutil not available
            return True
    
    def get_active_tasks(self) -> List[str]:
        """Get list of active tasks"""
        with self._lock:
            return list(self._active_tasks)

# --- 7. Advanced Data Validation ---
class ValidationResult:
    """Result of validation with detailed information"""
    
    def __init__(self):
        self.is_valid = True
        self.errors: List[str] = []
        self.warnings: List[str] = []
        self.info: List[str] = []
        self.metadata: Dict[str, Any] = {}
    
    def add_error(self, message: str):
        """Add error message"""
        self.errors.append(message)
        self.is_valid = False
    
    def add_warning(self, message: str):
        """Add warning message"""
        self.warnings.append(message)
    
    def add_info(self, message: str):
        """Add info message"""
        self.info.append(message)
    
    def merge(self, other: 'ValidationResult'):
        """Merge another validation result"""
        self.is_valid = self.is_valid and other.is_valid
        self.errors.extend(other.errors)
        self.warnings.extend(other.warnings)
        self.info.extend(other.info)
        self.metadata.update(other.metadata)

class DataValidator:
    """Advanced data validation with comprehensive checks"""
    
    def __init__(self, config: Configuration):
        self.config = config
        self._logger = LoggerFactory.get_logger('DataValidator')
    
    def validate_dataframe(self, df: pd.DataFrame, context: str = "data") -> ValidationResult:
        """Comprehensive dataframe validation"""
        result = ValidationResult()
        
        # Basic structure checks
        if df.empty:
            result.add_error(f"{context}: DataFrame is empty")
            return result
        
        if len(df.columns) == 0:
            result.add_error(f"{context}: No columns found")
            return result
        
        # Size checks
        if df.shape[0] > 1000000:
            result.add_warning(f"{context}: Large dataset ({df.shape[0]} rows), performance may be impacted")
        
        # Data type checks
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) == 0:
            result.add_warning(f"{context}: No numeric columns found")
        
        # Check for duplicates
        if df.index.duplicated().any():
            result.add_warning(f"{context}: Duplicate indices found")
        
        # Check for missing values
        missing_pct = (df.isnull().sum().sum() / (df.shape[0] * df.shape[1])) * 100
        if missing_pct > 50:
            result.add_warning(f"{context}: High percentage of missing values ({missing_pct:.1f}%)")
        
        # Check for constant columns
        for col in df.columns:
            if df[col].nunique() == 1:
                result.add_info(f"{context}: Column '{col}' has constant value")
        
        # Statistical checks for numeric columns
        for col in numeric_cols:
            series = df[col].dropna()
            if len(series) > 0:
                # Check for outliers
                mean = series.mean()
                std = series.std()
                outlier_threshold = self.config.get('analysis.outlier_std_threshold', 3)
                
                outliers = series[(series < mean - outlier_threshold * std) | 
                                 (series > mean + outlier_threshold * std)]
                
                if len(outliers) > len(series) * 0.1:
                    result.add_warning(
                        f"{context}: Column '{col}' has many outliers ({len(outliers)} values)"
                    )
                
                # Check for negative values in typically positive metrics
                positive_keywords = ['assets', 'revenue', 'sales', 'income', 'cash']
                if any(keyword in str(col).lower() for keyword in positive_keywords):
                    if (series < 0).any():
                        result.add_warning(f"{context}: Column '{col}' contains negative values")
        
        # Add metadata
        result.metadata['shape'] = df.shape
        result.metadata['columns'] = list(df.columns)
        result.metadata['dtypes'] = df.dtypes.to_dict()
        result.metadata['memory_usage'] = df.memory_usage(deep=True).sum()
        
        return result
    
    def validate_financial_data(self, df: pd.DataFrame) -> ValidationResult:
        """Validate financial statement data"""
        result = self.validate_dataframe(df, "financial_data")
        
        # Additional financial-specific checks
        required_keywords = ['asset', 'liability', 'equity', 'revenue', 'expense']
        found_keywords = []
        
        for keyword in required_keywords:
            if any(keyword in str(idx).lower() for idx in df.index):
                found_keywords.append(keyword)
        
        if len(found_keywords) < 2:
            result.add_warning(
                "Limited financial keywords found in data. "
                "Please verify the data contains financial statements."
            )
        
        # Check accounting equation (Assets = Liabilities + Equity)
        asset_rows = [idx for idx in df.index if 'total asset' in str(idx).lower()]
        liability_rows = [idx for idx in df.index if 'total liabilit' in str(idx).lower()]
        equity_rows = [idx for idx in df.index if 'total equity' in str(idx).lower()]
        
        if asset_rows and liability_rows and equity_rows:
            for col in df.select_dtypes(include=[np.number]).columns:
                try:
                    assets = df.loc[asset_rows[0], col]
                    liabilities = df.loc[liability_rows[0], col]
                    equity = df.loc[equity_rows[0], col]
                    
                    if not pd.isna(assets) and not pd.isna(liabilities) and not pd.isna(equity):
                        diff = abs(assets - (liabilities + equity))
                        tolerance = assets * 0.01  # 1% tolerance
                        
                        if diff > tolerance:
                            result.add_warning(
                                f"Accounting equation imbalance in {col}: "
                                f"Assets ({assets:,.0f}) â‰  Liabilities ({liabilities:,.0f}) "
                                f"+ Equity ({equity:,.0f})"
                            )
                except Exception:
                    pass
        
        return result

# --- 8. Advanced Pattern Matching System ---
class PatternMatcher:
    """Advanced pattern matching for financial metrics"""
    
    def __init__(self):
        self._patterns = self._build_patterns()
        self._logger = LoggerFactory.get_logger('PatternMatcher')
    
    def _build_patterns(self) -> Dict[str, List[re.Pattern]]:
        """Build comprehensive pattern library"""
        return {
            # Assets
            'total_assets': [
                re.compile(r'\btotal\s+assets?\b', re.IGNORECASE),
                re.compile(r'\bassets?\s+total\b', re.IGNORECASE),
                re.compile(r'\b(?:sum|total)\s+of\s+assets?\b', re.IGNORECASE),
            ],
            'current_assets': [
                re.compile(r'\bcurrent\s+assets?\b', re.IGNORECASE),
                re.compile(r'\bshort[\s-]?term\s+assets?\b', re.IGNORECASE),
                re.compile(r'\bliquid\s+assets?\b', re.IGNORECASE),
            ],
            'non_current_assets': [
                re.compile(r'\bnon[\s-]?current\s+assets?\b', re.IGNORECASE),
                re.compile(r'\blong[\s-]?term\s+assets?\b', re.IGNORECASE),
                re.compile(r'\bfixed\s+assets?\b', re.IGNORECASE),
            ],
            'cash': [
                re.compile(r'\bcash\b', re.IGNORECASE),
                re.compile(r'\bcash\s+(?:and|&)\s+cash\s+equivalents?\b', re.IGNORECASE),
                re.compile(r'\bcash\s+(?:and|&)\s+bank\b', re.IGNORECASE),
                re.compile(r'\bliquid\s+funds?\b', re.IGNORECASE),
            ],
            'inventory': [
                re.compile(r'\binventor(?:y|ies)\b', re.IGNORECASE),
                re.compile(r'\bstock\s+in\s+trade\b', re.IGNORECASE),
                re.compile(r'\bgoods?\b', re.IGNORECASE),
                re.compile(r'\bmaterials?\b', re.IGNORECASE),
            ],
            'receivables': [
                re.compile(r'\breceivables?\b', re.IGNORECASE),
                re.compile(r'\baccounts?\s+receivables?\b', re.IGNORECASE),
                re.compile(r'\btrade\s+receivables?\b', re.IGNORECASE),
                re.compile(r'\bdebtors?\b', re.IGNORECASE),
            ],
            
            # Liabilities
            'total_liabilities': [
                re.compile(r'\btotal\s+liabilit(?:y|ies)\b', re.IGNORECASE),
                re.compile(r'\bliabilit(?:y|ies)\s+total\b', re.IGNORECASE),
                re.compile(r'\b(?:sum|total)\s+of\s+liabilit(?:y|ies)\b', re.IGNORECASE),
            ],
            'current_liabilities': [
                re.compile(r'\bcurrent\s+liabilit(?:y|ies)\b', re.IGNORECASE),
                re.compile(r'\bshort[\s-]?term\s+liabilit(?:y|ies)\b', re.IGNORECASE),
            ],
            'non_current_liabilities': [
                re.compile(r'\bnon[\s-]?current\s+liabilit(?:y|ies)\b', re.IGNORECASE),
                re.compile(r'\blong[\s-]?term\s+liabilit(?:y|ies)\b', re.IGNORECASE),
            ],
            'debt': [
                re.compile(r'\bdebt\b', re.IGNORECASE),
                re.compile(r'\bborrowing\b', re.IGNORECASE),
                re.compile(r'\bloan\b', re.IGNORECASE),
                re.compile(r'\bdebenture\b', re.IGNORECASE),
            ],
            
            # Equity
            'total_equity': [
                re.compile(r'\btotal\s+equity\b', re.IGNORECASE),
                re.compile(r'\bshareholders?\s+equity\b', re.IGNORECASE),
                re.compile(r'\bstockholders?\s+equity\b', re.IGNORECASE),
                re.compile(r'\bnet\s+worth\b', re.IGNORECASE),
            ],
            'share_capital': [
                re.compile(r'\bshare\s+capital\b', re.IGNORECASE),
                re.compile(r'\bcapital\s+stock\b', re.IGNORECASE),
                re.compile(r'\bpaid[\s-]?up\s+capital\b', re.IGNORECASE),
                re.compile(r'\bequity\s+shares?\b', re.IGNORECASE),
            ],
            'retained_earnings': [
                re.compile(r'\bretained\s+earnings?\b', re.IGNORECASE),
                re.compile(r'\breserves?\s+(?:and|&)\s+surplus\b', re.IGNORECASE),
                re.compile(r'\baccumulated\s+profits?\b', re.IGNORECASE),
            ],
            
            # Income Statement
            'revenue': [
                re.compile(r'\brevenue\b', re.IGNORECASE),
                re.compile(r'\bsales?\b', re.IGNORECASE),
                re.compile(r'\bturnover\b', re.IGNORECASE),
                re.compile(r'\bincome\s+from\s+operations?\b', re.IGNORECASE),
                re.compile(r'\bnet\s+sales?\b', re.IGNORECASE),
            ],
            'cost_of_goods_sold': [
                re.compile(r'\bcost\s+of\s+goods?\s+sold\b', re.IGNORECASE),
                re.compile(r'\bcogs\b', re.IGNORECASE),
                re.compile(r'\bcost\s+of\s+sales?\b', re.IGNORECASE),
                re.compile(r'\bcost\s+of\s+revenue\b', re.IGNORECASE),
            ],
            'operating_expenses': [
                re.compile(r'\boperating\s+expenses?\b', re.IGNORECASE),
                re.compile(r'\bopex\b', re.IGNORECASE),
                re.compile(r'\badministrative\s+expenses?\b', re.IGNORECASE),
                re.compile(r'\bselling\s+(?:and|&)\s+distribution\b', re.IGNORECASE),
            ],
            'ebit': [
                re.compile(r'\bebit\b', re.IGNORECASE),
                re.compile(r'\bearnings?\s+before\s+interest\s+(?:and|&)\s+tax\b', re.IGNORECASE),
                re.compile(r'\boperating\s+(?:profit|income)\b', re.IGNORECASE),
            ],
            'net_income': [
                re.compile(r'\bnet\s+(?:income|profit|earnings?)\b', re.IGNORECASE),
                re.compile(r'\bprofit\s+after\s+tax\b', re.IGNORECASE),
                re.compile(r'\bpat\b', re.IGNORECASE),
                re.compile(r'\bprofit\s+for\s+the\s+(?:year|period)\b', re.IGNORECASE),
            ],
        }
    
    def find_matches(self, text: str, metric_type: str) -> List[Tuple[str, float]]:
        """Find pattern matches with confidence scores"""
        matches = []
        
        if metric_type not in self._patterns:
            return matches
        
        for pattern in self._patterns[metric_type]:
            if pattern.search(text):
                # Calculate confidence based on match quality
                match = pattern.search(text)
                confidence = self._calculate_confidence(text, match)
                matches.append((metric_type, confidence))
        
        return matches
    
    def _calculate_confidence(self, text: str, match: re.Match) -> float:
        """Calculate confidence score for a match"""
        # Base confidence
        confidence = 0.7
        
        # Exact match bonus
        if match.group(0).lower() == text.lower():
            confidence += 0.2
        
        # Position bonus (earlier matches are better)
        position_ratio = match.start() / len(text)
        confidence += (1 - position_ratio) * 0.1
        
        return min(confidence, 1.0)
    
    def classify_metric(self, text: str) -> Dict[str, float]:
        """Classify a metric into categories with confidence scores"""
        classifications = defaultdict(float)
        
        for metric_type in self._patterns:
            matches = self.find_matches(text, metric_type)
            for _, confidence in matches:
                classifications[metric_type] = max(
                    classifications[metric_type], 
                    confidence
                )
        
        return dict(classifications)

# --- 9. Simplified State Management ---
class SimpleState:
    """Simple state wrapper for session state"""
    
    @staticmethod
    def get(key: str, default: Any = None) -> Any:
        """Get value from session state"""
        return st.session_state.get(key, default)
    
    @staticmethod
    def set(key: str, value: Any):
        """Set value in session state"""
        st.session_state[key] = value
    
    @staticmethod
    def update(updates: Dict[str, Any]):
        """Batch update session state"""
        for key, value in updates.items():
            st.session_state[key] = value

# --- 10. Base Components with Dependency Injection ---
class Component(ABC):
    """Base component with lifecycle management"""
    
    def __init__(self, config: Configuration):
        self.config = config
        self._logger = LoggerFactory.get_logger(self.__class__.__name__)
        self._initialized = False
    
    def initialize(self):
        """Initialize component"""
        if not self._initialized:
            self._logger.info(f"Initializing {self.__class__.__name__}")
            self._do_initialize()
            self._initialized = True
    
    @abstractmethod
    def _do_initialize(self):
        """Actual initialization logic"""
        pass
    
    def cleanup(self):
        """Cleanup component"""
        if self._initialized:
            self._logger.info(f"Cleaning up {self.__class__.__name__}")
            self._do_cleanup()
            self._initialized = False
    
    def _do_cleanup(self):
        """Actual cleanup logic"""
        pass

# --- 11. Enhanced Security Module ---
class SecurityModule(Component):
    """Enhanced security with comprehensive validation"""
    
    def __init__(self, config: Configuration):
        super().__init__(config)
        self._sanitizer = None
        self._rate_limiter = defaultdict(deque)
        self._blocked_ips = set()
    
    def _do_initialize(self):
        """Initialize security components"""
        # Initialize HTML sanitizer
        self._allowed_tags = ['table', 'tr', 'td', 'th', 'tbody', 'thead', 'p', 'div', 'span', 'br']
        self._allowed_attributes = {
            '*': ['class', 'id'],
            'table': ['border', 'cellpadding', 'cellspacing'],
        }
    
    def validate_file_upload(self, file: UploadedFile) -> ValidationResult:
        """Comprehensive file validation"""
        result = ValidationResult()
        
        # Check file size
        max_size = self.config.get('app.max_file_size_mb', 10) * 1024 * 1024
        if file.size > max_size:
            result.add_error(f"File size ({file.size / 1024 / 1024:.1f}MB) exceeds limit ({max_size / 1024 / 1024}MB)")
            return result
        
        # Check file extension
        allowed_types = self.config.get('app.allowed_file_types', [])
        file_ext = Path(file.name).suffix.lower().lstrip('.')
        
        if file_ext not in allowed_types:
            result.add_error(f"File type '{file_ext}' not allowed. Allowed types: {', '.join(allowed_types)}")
            return result
        
        # Check file name for suspicious patterns
        suspicious_patterns = [
            r'\.\./', r'\.\.\\',  # Path traversal
            r'[<>:"|?*]',  # Invalid characters
            r'^\.',  # Hidden files
            r'\.(exe|bat|cmd|sh|ps1)$',  # Executable extensions
        ]
        
        for pattern in suspicious_patterns:
            if re.search(pattern, file.name, re.IGNORECASE):
                result.add_error(f"Suspicious file name pattern detected")
                return result
        
        # Content validation for HTML/XML files
        if file_ext in ['html', 'htm', 'xml']:
            content = self._read_file_safely(file)
            if content:
                validation = self._validate_html_content(content)
                result.merge(validation)
        
        return result
    
    def _read_file_safely(self, file: UploadedFile, max_bytes: int = 1024 * 1024) -> Optional[str]:
        """Safely read file content with size limit"""
        try:
            content = file.read(max_bytes)
            file.seek(0)  # Reset file pointer
            
            # Try to decode
            for encoding in ['utf-8', 'latin-1', 'cp1252']:
                try:
                    return content.decode(encoding)
                except UnicodeDecodeError:
                    continue
            
            return None
        except Exception as e:
            self._logger.error(f"Error reading file: {e}")
            return None
    
    def _validate_html_content(self, content: str) -> ValidationResult:
        """Validate HTML content for security issues"""
        result = ValidationResult()
        
        # Check for malicious patterns
        malicious_patterns = [
            (r'<script', 'JavaScript code detected'),
            (r'javascript:', 'JavaScript protocol detected'),
            (r'on\w+\s*=', 'Event handler detected'),
            (r'<iframe', 'IFrame detected'),
            (r'<object', 'Object tag detected'),
            (r'<embed', 'Embed tag detected'),
            (r'<link[^>]+href', 'External link detected'),
            (r'@import', 'CSS import detected'),
            (r'expression\s*\(', 'CSS expression detected'),
            (r'vbscript:', 'VBScript protocol detected'),
        ]
        
        content_lower = content.lower()
        for pattern, message in malicious_patterns:
            if re.search(pattern, content_lower):
                result.add_error(f"Security issue: {message}")
        
        # Check content size
        if len(content) > 10 * 1024 * 1024:  # 10MB
            result.add_warning("Large HTML content may impact performance")
        
        return result
    
    def sanitize_html(self, content: str) -> str:
        """Sanitize HTML content"""
        return bleach.clean(
            content,
            tags=self._allowed_tags,
            attributes=self._allowed_attributes,
            strip=True,
            strip_comments=True
        )
    
    def check_rate_limit(self, identifier: str, action: str, limit: int = 100, window: int = 60) -> bool:
        """Check rate limit for an action"""
        key = f"{identifier}:{action}"
        now = time.time()
        
        # Clean old entries
        self._rate_limiter[key] = deque(
            [t for t in self._rate_limiter[key] if now - t < window],
            maxlen=limit
        )
        
        # Check limit
        if len(self._rate_limiter[key]) >= limit:
            self._logger.warning(f"Rate limit exceeded for {key}")
            return False
        
        # Add current request
        self._rate_limiter[key].append(now)
        return True

# --- 12. Enhanced Data Processing Pipeline ---
class DataProcessor(Component):
    """Advanced data processing with pipeline architecture"""
    
    def __init__(self, config: Configuration):
        super().__init__(config)
        self._transformers = []
        self._validators = []
        self.resource_manager = None
    
    def _do_initialize(self):
        """Initialize processor"""
        self.resource_manager = ResourceManager(self.config)
        self._setup_pipeline()
    
    def _setup_pipeline(self):
        """Setup processing pipeline"""
        # Add transformers
        self._transformers = [
            self._clean_numeric_data,
            self._normalize_indices,
            self._interpolate_missing,
            self._detect_outliers,
        ]
        
        # Add validators
        validator = DataValidator(self.config)
        self._validators = [
            validator.validate_dataframe,
            validator.validate_financial_data,
        ]
    
    def process(self, df: pd.DataFrame, context: str = "data") -> Tuple[pd.DataFrame, ValidationResult]:
        """Process dataframe through pipeline"""
        result = ValidationResult()
        processed_df = df.copy()
        
        with self.resource_manager.acquire_worker(f"process_{context}"):
            # Validation phase
            for validator in self._validators:
                validation = validator(processed_df)
                result.merge(validation)
                
                if not validation.is_valid:
                    self._logger.warning(f"Validation failed in {context}")
                    break
            
            # Transformation phase
            if result.is_valid:
                for transformer in self._transformers:
                    try:
                        processed_df = transformer(processed_df)
                    except Exception as e:
                        result.add_error(f"Transformation error: {str(e)}")
                        self._logger.error(f"Error in transformer {transformer.__name__}: {e}")
                        break
        
        return processed_df, result
    
    def _clean_numeric_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Clean numeric data with advanced techniques"""
        df_clean = df.copy()
        
        for col in df.select_dtypes(include=['object']).columns:
            # Try to convert to numeric
            df_clean[col] = pd.to_numeric(df[col], errors='coerce')
            
            # If mostly non-numeric, keep as string
            if df_clean[col].isna().sum() > len(df) * 0.5:
                df_clean[col] = df[col]
        
        return df_clean
    
    def _normalize_indices(self, df: pd.DataFrame) -> pd.DataFrame:
        """Normalize index names"""
        df_norm = df.copy()
        
        # Clean index names
        if isinstance(df.index, pd.Index):
            df_norm.index = df.index.map(lambda x: str(x).strip())
        
        # Remove duplicate indices
        if df_norm.index.duplicated().any():
            df_norm = df_norm[~df_norm.index.duplicated(keep='first')]
            self._logger.warning("Removed duplicate indices")
        
        return df_norm
    
    def _interpolate_missing(self, df: pd.DataFrame) -> pd.DataFrame:
        """Interpolate missing values intelligently"""
        df_interp = df.copy()
        method = self.config.get('analysis.interpolation_method', 'linear')
        
        for col in df.select_dtypes(include=[np.number]).columns:
            if df[col].isna().any():
                # Only interpolate if enough data points
                non_na_count = df[col].notna().sum()
                min_points = self.config.get('analysis.min_data_points', 3)
                
                if non_na_count >= min_points:
                    df_interp[col] = df[col].interpolate(method=method, limit_direction='both')
        
        return df_interp
    
    def _detect_outliers(self, df: pd.DataFrame) -> pd.DataFrame:
        """Detect and optionally handle outliers"""
        df_clean = df.copy()
        outlier_threshold = self.config.get('analysis.outlier_std_threshold', 3)
        
        for col in df.select_dtypes(include=[np.number]).columns:
            series = df[col].dropna()
            if len(series) > 3:
                mean = series.mean()
                std = series.std()
                
                if std > 0:
                    # Mark outliers
                    lower_bound = mean - outlier_threshold * std
                    upper_bound = mean + outlier_threshold * std
                    
                    outliers = (series < lower_bound) | (series > upper_bound)
                    if outliers.any():
                        self._logger.info(f"Found {outliers.sum()} outliers in {col}")
                        
                        # Store outlier information in session state
                        outlier_indices = series[outliers].index.tolist()
                        SimpleState.set(f"outliers_{col}", outlier_indices)
        
        return df_clean

# --- NEW: Performance Optimization Classes ---
from numba import jit

@jit(nopython=True)
def calculate_moving_metrics(values: np.ndarray, window: int) -> np.ndarray:
    """Optimized moving average calculation with Numba"""
    n = len(values)
    result = np.empty(n)
    result[:window-1] = np.nan
    
    for i in range(window-1, n):
        result[i] = np.mean(values[i-window+1:i+1])
    
    return result

class ChunkedDataProcessor:
    """Process large datasets in chunks to manage memory"""
    
    def __init__(self, chunk_size: int = 10000):
        self.chunk_size = chunk_size
    
    def process_large_file(self, file_path: str, processing_func: Callable) -> pd.DataFrame:
        """Process large file in chunks"""
        chunks = []
        
        # Read and process in chunks
        for chunk in pd.read_csv(file_path, chunksize=self.chunk_size):
            processed_chunk = processing_func(chunk)
            chunks.append(processed_chunk)
        
        # Combine chunks efficiently
        return pd.concat(chunks, ignore_index=True)

class LazyDataFrame:
    """Lazy loading wrapper for DataFrames"""
    
    def __init__(self, loader_func: Callable):
        self._loader = loader_func
        self._data = None
    
    @property
    def data(self) -> pd.DataFrame:
        if self._data is None:
            self._data = self._loader()
        return self._data
    
    def invalidate(self):
        """Invalidate cached data"""
        self._data = None

# --- 13. Enhanced Financial Analysis Engine (with New Methods) ---
class FinancialAnalysisEngine(Component):
    """Core financial analysis engine with advanced features"""
    
    def __init__(self, config: Configuration):
        super().__init__(config)
        self.pattern_matcher = PatternMatcher()
        self.cache = AdvancedCache()
    
    def _do_initialize(self):
        """Initialize analysis components"""
        # Load any required models or data
        pass
    
    def analyze_financial_statements(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Comprehensive financial statement analysis"""
        cache_key = self._generate_cache_key(df)
        cached_result = self.cache.get(cache_key)
        
        if cached_result:
            self._logger.info("Returning cached analysis")
            return cached_result
        
        analysis = {
            'summary': self._generate_summary(df),
            'metrics': self._extract_key_metrics(df),
            'ratios': self._calculate_ratios(df),
            'trends': self._analyze_trends(df),
            'quality_score': self._calculate_quality_score(df),
            'insights': self._generate_insights(df),
            # NEW: Added new analysis methods
            'dupont': self._calculate_dupont_analysis(df, self._extract_key_metrics(df)),
            'cash_flow_quality': self._analyze_cash_flow_quality(df, self._extract_key_metrics(df)),
            'altman_z': self._calculate_altman_z_score(df, self._extract_key_metrics(df)),
        }
        
        # Cache the result
        self.cache.set(cache_key, analysis, ttl=3600)
        
        return analysis
    
    def _generate_cache_key(self, df: pd.DataFrame) -> str:
        """Generate cache key for dataframe"""
        # Use shape and sample of data for key
        key_parts = [
            str(df.shape),
            str(df.index[:5].tolist()),
            str(df.columns[:5].tolist()),
            str(df.iloc[:5, :5].values.tolist()) if df.shape[0] >= 5 and df.shape[1] >= 5 else ""
        ]
        
        key_string = "|".join(key_parts)
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def _generate_summary(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Generate summary statistics"""
        numeric_df = df.select_dtypes(include=[np.number])
        
        if numeric_df.empty:
            return {'error': 'No numeric data found'}
        
        summary = {
            'total_metrics': len(df),
            'years_covered': len(numeric_df.columns),
            'year_range': f"{numeric_df.columns[0]} - {numeric_df.columns[-1]}" if len(numeric_df.columns) > 0 else "N/A",
            'completeness': (numeric_df.notna().sum().sum() / numeric_df.size) * 100,
            'key_statistics': {}
        }
        
        # Calculate key statistics
        for col in numeric_df.columns[-3:]:  # Last 3 years
            summary['key_statistics'][str(col)] = {
                'mean': numeric_df[col].mean(),
                'median': numeric_df[col].median(),
                'std': numeric_df[col].std(),
                'min': numeric_df[col].min(),
                'max': numeric_df[col].max()
            }
        
        return summary
    
    def _extract_key_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Extract and classify key financial metrics"""
        metrics = {}
        
        for idx in df.index:
            classifications = self.pattern_matcher.classify_metric(str(idx))
            if classifications:
                top_classification = max(classifications.items(), key=lambda x: x[1])
                metric_type, confidence = top_classification
                
                if confidence > 0.5:
                    if metric_type not in metrics:
                        metrics[metric_type] = []
                    
                    metrics[metric_type].append({
                        'name': str(idx),
                        'confidence': confidence,
                        'values': df.loc[idx].to_dict()
                    })
        
        return metrics
    
    def _calculate_ratios(self, df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
        """Calculate financial ratios with error handling"""
        ratios = {}
        
        # Extract key metrics using pattern matching
        metrics = self._extract_key_metrics(df)
        
        # Helper function to check if value exists and ensure it's 1-dimensional
        def get_clean_metric(metric):
            if metric is None:
                return None
            if isinstance(metric, pd.DataFrame):
                # If DataFrame, take first row
                metric = metric.iloc[0]
            if isinstance(metric, pd.Series) and metric.empty:
                return None
            return metric
        
        # Liquidity Ratios
        try:
            liquidity_data = {}
            
            # Current Ratio
            current_assets = get_clean_metric(self._get_metric_value(df, metrics, 'current_assets'))
            current_liabilities = get_clean_metric(self._get_metric_value(df, metrics, 'current_liabilities'))
            
            if current_assets is not None and current_liabilities is not None:
                liquidity_data['Current Ratio'] = current_assets / current_liabilities.replace(0, np.nan)
            
            # Quick Ratio
            inventory = get_clean_metric(self._get_metric_value(df, metrics, 'inventory'))
            if current_assets is not None and inventory is not None and current_liabilities is not None:
                liquidity_data['Quick Ratio'] = (current_assets - inventory) / current_liabilities.replace(0, np.nan)
            
            if liquidity_data:
                liquidity_df = pd.DataFrame(liquidity_data)
                ratios['Liquidity'] = liquidity_df.T
                
        except Exception as e:
            self._logger.error(f"Error calculating liquidity ratios: {e}")
        
        # Profitability Ratios
        try:
            profitability_data = {}
            
            # Net Profit Margin
            net_income = get_clean_metric(self._get_metric_value(df, metrics, 'net_income'))
            revenue = get_clean_metric(self._get_metric_value(df, metrics, 'revenue'))
            
            if net_income is not None and revenue is not None:
                profitability_data['Net Profit Margin %'] = (net_income / revenue.replace(0, np.nan)) * 100
            
            # ROA
            total_assets = get_clean_metric(self._get_metric_value(df, metrics, 'total_assets'))
            if net_income is not None and total_assets is not None:
                profitability_data['Return on Assets %'] = (net_income / total_assets.replace(0, np.nan)) * 100
            
            # ROE
            total_equity = get_clean_metric(self._get_metric_value(df, metrics, 'total_equity'))
            if net_income is not None and total_equity is not None:
                profitability_data['Return on Equity %'] = (net_income / total_equity.replace(0, np.nan)) * 100
            
            if profitability_data:
                profitability_df = pd.DataFrame(profitability_data)
                ratios['Profitability'] = profitability_df.T
                
        except Exception as e:
            self._logger.error(f"Error calculating profitability ratios: {e}")
        
        # Leverage Ratios
        try:
            leverage_data = {}
            
            # Debt to Equity
            total_liabilities = get_clean_metric(self._get_metric_value(df, metrics, 'total_liabilities'))
            if total_liabilities is not None and total_equity is not None:
                leverage_data['Debt to Equity'] = total_liabilities / total_equity.replace(0, np.nan)
            
            # Debt Ratio
            if total_liabilities is not None and total_assets is not None:
                leverage_data['Debt Ratio'] = total_liabilities / total_assets.replace(0, np.nan)
            
            if leverage_data:
                leverage_df = pd.DataFrame(leverage_data)
                ratios['Leverage'] = leverage_df.T
                
        except Exception as e:
            self._logger.error(f"Error calculating leverage ratios: {e}")
        
        return ratios
    
    def _get_metric_value(self, df: pd.DataFrame, metrics: Dict, metric_type: str) -> Optional[pd.Series]:
        """Get metric value from dataframe with fallback"""
        if metric_type in metrics and metrics[metric_type]:
            # Get the highest confidence match
            best_match = max(metrics[metric_type], key=lambda x: x['confidence'])
            metric_name = best_match['name']
            
            if metric_name in df.index:
                result = df.loc[metric_name]
                # If multiple rows match (DataFrame), take the first one
                if isinstance(result, pd.DataFrame):
                    self._logger.warning(f"Multiple rows found for {metric_name}, taking first")
                    result = result.iloc[0]
                return result
        
        return None
    
    def _analyze_trends(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Analyze trends in financial data"""
        trends = {}
        numeric_df = df.select_dtypes(include=[np.number])
        
        if len(numeric_df.columns) < 2:
            return {'error': 'Insufficient data for trend analysis'}
        
        for idx in numeric_df.index:
            series = numeric_df.loc[idx]
            
            # Handle case where loc returns a DataFrame (duplicate indices)
            if isinstance(series, pd.DataFrame):
                self._logger.warning(f"Multiple rows found for {idx}, taking first")
                series = series.iloc[0]
            
            series = series.dropna()
            
            if len(series) >= 3:
                # Calculate trend metrics
                years = np.arange(len(series))
                values = series.values
                
                # Linear regression - polyfit returns coefficients
                coefficients = np.polyfit(years, values, 1)
                slope = coefficients[0]  # First coefficient is slope
                intercept = coefficients[1]  # Second coefficient is intercept
                
                # Compound Annual Growth Rate (CAGR)
                try:
                    first_value = series.iloc[0]
                    last_value = series.iloc[-1]
                    
                    # Convert to scalar properly
                    if hasattr(first_value, 'item'):
                        first_value = first_value.item()
                    elif isinstance(first_value, np.ndarray):
                        first_value = first_value.flat[0]
                    else:
                        first_value = float(first_value)
                        
                    if hasattr(last_value, 'item'):
                        last_value = last_value.item()
                    elif isinstance(last_value, np.ndarray):
                        last_value = last_value.flat[0]
                    else:
                        last_value = float(last_value)
                    
                    if first_value > 0 and last_value > 0:
                        years_diff = len(series) - 1
                        cagr = ((last_value / first_value) ** (1 / years_diff) - 1) * 100
                    else:
                        cagr = None
                        
                except Exception as e:
                    self._logger.warning(f"Could not calculate CAGR for {idx}: {e}")
                    cagr = None
                
                # Volatility
                try:
                    volatility = series.pct_change().std() * 100
                    if pd.isna(volatility):
                        volatility = 0
                    else:
                        # Ensure it's a scalar
                        if hasattr(volatility, 'item'):
                            volatility = volatility.item()
                        elif isinstance(volatility, np.ndarray):
                            volatility = volatility.flat[0]
                        else:
                            volatility = float(volatility)
                except Exception:
                    volatility = 0
                
                # Ensure slope and intercept are scalars
                if hasattr(slope, 'item'):
                    slope = slope.item()
                elif isinstance(slope, np.ndarray):
                    slope = slope.flat[0]
                else:
                    slope = float(slope)
                    
                if hasattr(intercept, 'item'):
                    intercept = intercept.item()
                elif isinstance(intercept, np.ndarray):
                    intercept = intercept.flat[0]
                else:
                    intercept = float(intercept)
                
                trends[str(idx)] = {
                    'slope': slope,
                    'direction': 'increasing' if slope > 0 else 'decreasing',
                    'cagr': cagr,
                    'volatility': volatility,
                    'r_squared': self._calculate_r_squared(years, values, slope, intercept)
                }
        
        return trends
    
    def _calculate_r_squared(self, x: np.ndarray, y: np.ndarray, slope: float, intercept: float) -> float:
        """Calculate R-squared for linear regression"""
        y_pred = slope * x + intercept
        ss_res = np.sum((y - y_pred) ** 2)
        ss_tot = np.sum((y - np.mean(y)) ** 2)
        
        return 1 - (ss_res / ss_tot) if ss_tot > 0 else 0
    
    def _calculate_quality_score(self, df: pd.DataFrame) -> float:
        """Calculate data quality score"""
        scores = []
        
        # Completeness score
        completeness = (df.notna().sum().sum() / df.size) * 100
        scores.append(completeness)
        
        # Consistency score (check for reasonable values)
        numeric_df = df.select_dtypes(include=[np.number])
        if not numeric_df.empty:
            # Check for negative values in typically positive metrics
            positive_metrics = ['assets', 'revenue', 'equity']
            consistency_score = 100
            
            for idx in numeric_df.index:
                if any(keyword in str(idx).lower() for keyword in positive_metrics):
                    # Get the row data
                    row_data = numeric_df.loc[idx]
                    
                    # Handle case where loc returns a DataFrame (duplicate indices)
                    if isinstance(row_data, pd.DataFrame):
                        row_data = row_data.iloc[0]
                    
                    # Count negative values
                    negative_count = (row_data < 0).sum()
                    
                    # Ensure negative_count is a scalar
                    if hasattr(negative_count, 'item'):
                        negative_count = negative_count.item()
                    elif isinstance(negative_count, np.ndarray):
                        negative_count = int(negative_count)
                    else:
                        negative_count = int(negative_count)
                    
                    if negative_count > 0:
                        consistency_score -= (negative_count / len(numeric_df.columns)) * 20
            
            scores.append(max(0, consistency_score))
        
        # Temporal consistency (year-over-year changes)
        if len(numeric_df.columns) > 1:
            temporal_score = 100
            extreme_changes = 0
            
            for idx in numeric_df.index:
                series = numeric_df.loc[idx]
                
                # Handle case where loc returns a DataFrame (duplicate indices)
                if isinstance(series, pd.DataFrame):
                    series = series.iloc[0]
                
                series = series.dropna()
                
                if len(series) > 1:
                    pct_changes = series.pct_change().dropna()
                    # Flag changes over 200%
                    extreme_count = (pct_changes.abs() > 2).sum()
                    
                    # Ensure extreme_count is a scalar
                    if hasattr(extreme_count, 'item'):
                        extreme_count = extreme_count.item()
                    elif isinstance(extreme_count, np.ndarray):
                        extreme_count = int(extreme_count)
                    else:
                        extreme_count = int(extreme_count)
                    
                    extreme_changes += extreme_count
            
            total_changes = len(numeric_df) * (len(numeric_df.columns) - 1)
            if total_changes > 0:
                temporal_score -= (extreme_changes / total_changes) * 50
            
            scores.append(max(0, temporal_score))
        
        return sum(scores) / len(scores) if scores else 0
    
    def _generate_insights(self, df: pd.DataFrame) -> List[str]:
        """Generate actionable insights from analysis"""
        insights = []
        
        # Get calculated ratios
        ratios = self._calculate_ratios(df)
        
        # Liquidity insights
        if 'Liquidity' in ratios and 'Current Ratio' in ratios['Liquidity'].index:
            current_ratios = ratios['Liquidity'].loc['Current Ratio'].dropna()
            if len(current_ratios) > 0:
                latest_cr = current_ratios.iloc[-1]
                if latest_cr < 1:
                    insights.append(f"âš ï¸ Low current ratio ({latest_cr:.2f}) indicates potential liquidity issues")
                elif latest_cr > 3:
                    insights.append(f"ðŸ’¡ High current ratio ({latest_cr:.2f}) suggests excess idle assets")
        
        # Profitability insights
        if 'Profitability' in ratios and 'Net Profit Margin %' in ratios['Profitability'].index:
            npm = ratios['Profitability'].loc['Net Profit Margin %'].dropna()
            if len(npm) > 1:
                trend = 'improving' if npm.iloc[-1] > npm.iloc[0] else 'declining'
                insights.append(f"ðŸ“Š Net profit margin is {trend} ({npm.iloc[0]:.1f}% â†’ {npm.iloc[-1]:.1f}%)")
        
        # Leverage insights
        if 'Leverage' in ratios and 'Debt to Equity' in ratios['Leverage'].index:
            de_ratio = ratios['Leverage'].loc['Debt to Equity'].dropna()
            if len(de_ratio) > 0:
                latest_de = de_ratio.iloc[-1]
                if latest_de > 2:
                    insights.append(f"âš ï¸ High debt-to-equity ratio ({latest_de:.2f}) indicates high leverage")
        
        # Growth insights
        trends = self._analyze_trends(df)
        
        # Revenue growth
        revenue_trends = [v for k, v in trends.items() if 'revenue' in k.lower()]
        if revenue_trends and revenue_trends[0].get('cagr') is not None:
            cagr = revenue_trends[0]['cagr']
            if cagr > 20:
                insights.append(f"ðŸš€ Strong revenue growth (CAGR: {cagr:.1f}%)")
            elif cagr < 0:
                insights.append(f"ðŸ“‰ Declining revenue (CAGR: {cagr:.1f}%)")
        
        # Data quality
        quality_score = self._calculate_quality_score(df)
        if quality_score < 70:
            insights.append(f"âš ï¸ Data quality score is low ({quality_score:.0f}%), results may be less reliable")
        
        return insights

# --- NEW: Additional Analysis Methods in FinancialAnalysisEngine ---
    def _calculate_dupont_analysis(self, df: pd.DataFrame, metrics: Dict) -> pd.DataFrame:
        """Enhanced DuPont Analysis with 5-factor decomposition"""
        dupont_data = {}
        
        # Get required metrics
        net_income = self._get_metric_value(df, metrics, 'net_income')
        revenue = self._get_metric_value(df, metrics, 'revenue')
        total_assets = self._get_metric_value(df, metrics, 'total_assets')
        total_equity = self._get_metric_value(df, metrics, 'total_equity')
        ebit = self._get_metric_value(df, metrics, 'ebit')
        
        if all(x is not None for x in [net_income, revenue, total_assets, total_equity, ebit]):
            # 5-Factor DuPont
            dupont_data['Tax Burden'] = net_income / ebit.replace(0, np.nan)
            dupont_data['Interest Burden'] = ebit / ebit.replace(0, np.nan)  # Simplified
            dupont_data['Operating Margin'] = (ebit / revenue.replace(0, np.nan)) * 100
            dupont_data['Asset Turnover'] = revenue / total_assets.replace(0, np.nan)
            dupont_data['Equity Multiplier'] = total_assets / total_equity.replace(0, np.nan)
            dupont_data['ROE (DuPont)'] = (
                dupont_data['Tax Burden'] * 
                dupont_data['Operating Margin'] / 100 * 
                dupont_data['Asset Turnover'] * 
                dupont_data['Equity Multiplier']
            ) * 100
        
        return pd.DataFrame(dupont_data).T
    
    def _analyze_cash_flow_quality(self, df: pd.DataFrame, metrics: Dict) -> Dict[str, Any]:
        """Analyze cash flow quality and sustainability"""
        analysis = {}
        
        # Get cash flow metrics
        operating_cf = self._get_metric_value(df, metrics, 'operating_cash_flow')
        net_income = self._get_metric_value(df, metrics, 'net_income')
        capex = self._get_metric_value(df, metrics, 'capital_expenditure')
        
        if operating_cf is not None and net_income is not None:
            # Cash Flow Quality Ratio
            analysis['cf_quality_ratio'] = operating_cf / net_income.replace(0, np.nan)
            
            # Free Cash Flow Yield
            if capex is not None:
                free_cf = operating_cf - capex
                market_cap = self._get_metric_value(df, metrics, 'market_cap')
                if market_cap is not None:
                    analysis['fcf_yield'] = (free_cf / market_cap.replace(0, np.nan)) * 100
            
            # Cash Conversion Cycle components
            analysis['cash_conversion_cycle'] = self._calculate_cash_conversion_cycle(df, metrics)
        
        return analysis
    
    def _calculate_altman_z_score(self, df: pd.DataFrame, metrics: Dict) -> pd.DataFrame:
        """Calculate Altman Z-Score for bankruptcy prediction"""
        z_scores = {}
        
        # Get required metrics
        working_capital = self._get_working_capital(df, metrics)
        total_assets = self._get_metric_value(df, metrics, 'total_assets')
        retained_earnings = self._get_metric_value(df, metrics, 'retained_earnings')
        ebit = self._get_metric_value(df, metrics, 'ebit')
        market_cap = self._get_metric_value(df, metrics, 'market_cap')
        total_liabilities = self._get_metric_value(df, metrics, 'total_liabilities')
        revenue = self._get_metric_value(df, metrics, 'revenue')
        
        if all(x is not None for x in [working_capital, total_assets, retained_earnings, 
                                        ebit, total_liabilities, revenue]):
            # Z-Score components
            x1 = (working_capital / total_assets) * 1.2
            x2 = (retained_earnings / total_assets) * 1.4
            x3 = (ebit / total_assets) * 3.3
            x4 = (market_cap / total_liabilities) * 0.6 if market_cap is not None else 0
            x5 = (revenue / total_assets) * 1.0
            
            z_scores['Z-Score'] = x1 + x2 + x3 + x4 + x5
            z_scores['Risk Level'] = z_scores['Z-Score'].apply(
                lambda x: 'Safe' if x > 2.99 else 'Grey Zone' if x > 1.81 else 'Distress'
            )
        
        return pd.DataFrame(z_scores)
    
    def _advanced_peer_comparison(self, company_data: pd.DataFrame, peer_data: List[pd.DataFrame]) -> Dict:
        """Advanced peer comparison with percentile rankings"""
        comparison = {}
        
        # Calculate key metrics for all companies
        all_metrics = []
        for data in [company_data] + peer_data:
            metrics = self._calculate_key_metrics(data)
            all_metrics.append(metrics)
        
        # Create comparison matrix
        metric_names = ['ROE', 'ROA', 'Current Ratio', 'Debt to Equity', 'Net Margin']
        
        for metric in metric_names:
            values = [m.get(metric, np.nan) for m in all_metrics]
            company_value = values[0]
            peer_values = values[1:]
            
            if not np.isnan(company_value) and len(peer_values) > 0:
                comparison[metric] = {
                    'company_value': company_value,
                    'peer_mean': np.nanmean(peer_values),
                    'peer_median': np.nanmedian(peer_values),
                    'percentile_rank': stats.percentileofscore(peer_values, company_value),
                    'z_score': (company_value - np.nanmean(peer_values)) / np.nanstd(peer_values)
                }
        
        return comparison
    
    def _get_working_capital(self, df: pd.DataFrame, metrics: Dict) -> Optional[pd.Series]:
        """Calculate working capital"""
        current_assets = self._get_metric_value(df, metrics, 'current_assets')
        current_liabilities = self._get_metric_value(df, metrics, 'current_liabilities')
        if current_assets is not None and current_liabilities is not None:
            return current_assets - current_liabilities
        return None
    
    def _calculate_cash_conversion_cycle(self, df: pd.DataFrame, metrics: Dict) -> Optional[pd.Series]:
        """Calculate cash conversion cycle"""
        inventory = self._get_metric_value(df, metrics, 'inventory')
        receivables = self._get_metric_value(df, metrics, 'receivables')
        payables = self._get_metric_value(df, metrics, 'payables')
        revenue = self._get_metric_value(df, metrics, 'revenue')
        cogs = self._get_metric_value(df, metrics, 'cost_of_goods_sold')
        
        if all(x is not None for x in [inventory, receivables, payables, revenue, cogs]):
            dio = (inventory / (cogs / 365))
            dso = (receivables / (revenue / 365))
            dpo = (payables / (cogs / 365))
            return dio + dso - dpo
        return None
    
    def _calculate_key_metrics(self, df: pd.DataFrame) -> Dict[str, float]:
        """Calculate key metrics for peer comparison"""
        metrics = {}
        # Implement calculations for ROE, ROA, etc.
        # Example:
        net_income = df.get('Net Income', pd.Series([0])).mean()
        equity = df.get('Total Equity', pd.Series([1])).mean()
        metrics['ROE'] = (net_income / equity) * 100
        # Add other metrics similarly
        return metrics

# --- 14. AI-Enhanced Mapping System ---
class AIMapper(Component):
    """AI-powered mapping with fallback mechanisms"""
    def __init__(self, config: Configuration):
    super().__init__(config)
    self.model = None
    self.embeddings_cache = AdvancedCache(max_size_mb=50)
    self.fallback_mapper = None
    
    def _do_initialize(self):
        """Initialize AI components"""
        if not self.config.get('ai.enabled', True):
            self._logger.info("AI mapping disabled in configuration")
            return
        
        try:
            # Only try to load if available
            if SENTENCE_TRANSFORMER_AVAILABLE:
                model_name = self.config.get('ai.model_name', 'all-MiniLM-L6-v2')
                self.model = SentenceTransformer(model_name)
                self._logger.info(f"Loaded AI model: {model_name}")
                
                # Pre-compute standard embeddings
                self._precompute_standard_embeddings()
            else:
                self._logger.warning("Sentence transformers not available, using fallback")
                
        except Exception as e:
            self._logger.error(f"Failed to initialize AI model: {e}")
        
        # Initialize fallback
        self.fallback_mapper = FuzzyMapper(self.config)
        self.fallback_mapper.initialize()
    
    def _precompute_standard_embeddings(self):
        """Pre-compute embeddings for standard metrics"""
        if not self.model:
            return
        
        standard_metrics = {
            'Total Assets': ['total assets', 'sum of assets', 'asset total'],
            'Total Liabilities': ['total liabilities', 'sum of liabilities', 'liability total'],
            'Total Equity': ['total equity', 'shareholders equity', 'net worth'],
            'Revenue': ['revenue', 'sales', 'turnover', 'income from operations'],
            'Net Income': ['net income', 'net profit', 'profit after tax', 'earnings'],
            'Current Assets': ['current assets', 'short term assets', 'liquid assets'],
            'Current Liabilities': ['current liabilities', 'short term liabilities'],
            'Cash': ['cash', 'cash and cash equivalents', 'liquid funds'],
            'Inventory': ['inventory', 'stock', 'goods'],
            'Receivables': ['receivables', 'accounts receivable', 'trade receivables', 'debtors'],
        }
        
        for metric, descriptions in standard_metrics.items():
            combined_text = ' '.join(descriptions)
            embedding = self._get_embedding(combined_text)
            if embedding is not None:
                SimpleState.set(f"standard_embedding_{metric}", embedding)
    
    def _get_embedding(self, text: str) -> Optional[np.ndarray]:
        """Get embedding with caching"""
        if not self.model:
            return None
        
        # Check cache
        cache_key = f"embedding_{hashlib.md5(text.encode()).hexdigest()}"
        cached = self.embeddings_cache.get(cache_key)
        if cached is not None:
            return cached
        
        try:
            # Compute embedding
            embedding = self.model.encode(
                text, 
                convert_to_numpy=True,
                show_progress_bar=False
            )
            
            # Cache it
            self.embeddings_cache.set(cache_key, embedding)
            
            return embedding
            
        except Exception as e:
            self._logger.error(f"Error computing embedding: {e}")
            return None
    
    def map_metrics(self, source_metrics: List[str], 
                   target_metrics: Optional[List[str]] = None,
                   confidence_threshold: Optional[float] = None) -> Dict[str, Any]:
        """Map source metrics to target metrics"""
        
        if not self.model:
            # Use fallback
            return self.fallback_mapper.map_metrics(source_metrics, target_metrics)
        
        if confidence_threshold is None:
            confidence_threshold = self.config.get('ai.similarity_threshold', 0.6)
        
        # Default target metrics
        if target_metrics is None:
            target_metrics = self._get_standard_metrics()
        
        mappings = {}
        confidence_scores = {}
        suggestions = {}
        unmapped = []
        
        # Batch process for efficiency
        batch_size = self.config.get('ai.batch_size', 32)
        
        for i in range(0, len(source_metrics), batch_size):
            batch = source_metrics[i:i + batch_size]
            
            try:
                # Get embeddings for batch
                source_embeddings = []
                valid_sources = []
                
                for metric in batch:
                    embedding = self._get_embedding(str(metric).lower())
                    if embedding is not None:
                        source_embeddings.append(embedding)
                        valid_sources.append(metric)
                
                if not source_embeddings:
                    unmapped.extend(batch)
                    continue
                
                # Get target embeddings
                target_embeddings = []
                valid_targets = []
                
                for target in target_metrics:
                    # Check for pre-computed embedding
                    embedding = SimpleState.get(f"standard_embedding_{target}")
                    
                    if embedding is None:
                        embedding = self._get_embedding(target.lower())
                    
                    if embedding is not None:
                        target_embeddings.append(embedding)
                        valid_targets.append(target)
                
                if not target_embeddings:
                    unmapped.extend(valid_sources)
                    continue
                
                # Compute similarities
                source_matrix = np.vstack(source_embeddings)
                target_matrix = np.vstack(target_embeddings)
                
                similarities = cosine_similarity(source_matrix, target_matrix)
                
                # Process results
                for idx, source in enumerate(valid_sources):
                    sim_scores = similarities[idx]
                    top_indices = np.argsort(sim_scores)[::-1][:3]
                    
                    top_matches = [
                        (valid_targets[i], sim_scores[i]) 
                        for i in top_indices
                    ]
                    
                    best_target, best_score = top_matches[0]
                    
                    if best_score >= confidence_threshold:
                        mappings[source] = best_target
                        confidence_scores[source] = float(best_score)
                    else:
                        unmapped.append(source)
                    
                    suggestions[source] = [
                        (target, float(score)) 
                        for target, score in top_matches
                    ]
                    
            except Exception as e:
                self._logger.error(f"Error in batch mapping: {e}")
                unmapped.extend(batch)
        
        return {
            'mappings': mappings,
            'confidence_scores': confidence_scores,
            'suggestions': suggestions,
            'unmapped_metrics': unmapped,
            'method': 'ai' if self.model else 'fuzzy'
        }
    
    def _get_standard_metrics(self) -> List[str]:
        """Get list of standard financial metrics"""
        return [
            'Total Assets', 'Current Assets', 'Non-current Assets',
            'Cash and Cash Equivalents', 'Inventory', 'Trade Receivables',
            'Property Plant and Equipment', 'Total Liabilities',
            'Current Liabilities', 'Non-current Liabilities',
            'Total Equity', 'Share Capital', 'Retained Earnings',
            'Revenue', 'Cost of Goods Sold', 'Gross Profit',
            'Operating Expenses', 'Operating Income', 'Net Income',
            'Earnings Per Share', 'Operating Cash Flow',
            'Investing Cash Flow', 'Financing Cash Flow'
        ]

# --- 15. Fuzzy Mapping Fallback ---
class FuzzyMapper(Component):
    """Fuzzy string matching for metric mapping"""
    
    def _do_initialize(self):
        """Initialize fuzzy mapper"""
        pass
    
    def map_metrics(self, source_metrics: List[str], 
                   target_metrics: Optional[List[str]] = None) -> Dict[str, Any]:
        """Map metrics using fuzzy string matching"""
        
        if target_metrics is None:
            target_metrics = self._get_standard_metrics()
        
        mappings = {}
        confidence_scores = {}
        suggestions = {}
        unmapped = []
        
        for source in source_metrics:
            source_lower = str(source).lower()
            
            # Calculate fuzzy scores
            scores = []
            for target in target_metrics:
                score = fuzz.token_sort_ratio(source_lower, target.lower()) / 100.0
                scores.append((target, score))
            
            # Sort by score
            scores.sort(key=lambda x: x[1], reverse=True)
            
            # Get best match
            if scores and scores[0][1] > 0.7:
                mappings[source] = scores[0][0]
                confidence_scores[source] = scores[0][1]
            else:
                unmapped.append(source)
            
            # Store top 3 suggestions
            suggestions[source] = scores[:3]
        
        return {
            'mappings': mappings,
            'confidence_scores': confidence_scores,
            'suggestions': suggestions,
            'unmapped_metrics': unmapped,
            'method': 'fuzzy'
        }
    
    def _get_standard_metrics(self) -> List[str]:
        """Get standard metrics list"""
        # Same as AIMapper
        return [
            'Total Assets', 'Current Assets', 'Non-current Assets',
            'Cash and Cash Equivalents', 'Inventory', 'Trade Receivables',
            'Property Plant and Equipment', 'Total Liabilities',
            'Current Liabilities', 'Non-current Liabilities',
            'Total Equity', 'Share Capital', 'Retained Earnings',
            'Revenue', 'Cost of Goods Sold', 'Gross Profit',
            'Operating Expenses', 'Operating Income', 'Net Income',
            'Earnings Per Share', 'Operating Cash Flow',
            'Investing Cash Flow', 'Financing Cash Flow'
        ]

# --- 16. Enhanced Penman-Nissim Analyzer (FIXED) ---
class EnhancedPenmanNissimAnalyzer:
    """Enhanced Penman-Nissim analyzer with flexible initialization"""
    
    def __init__(self, df: pd.DataFrame, mappings: Dict[str, str]):
        self.df = df
        self.mappings = mappings
        self.logger = LoggerFactory.get_logger('PenmanNissim')
        
        # Initialize core analyzer with proper handling
        self._initialize_core_analyzer()
    
    def _initialize_core_analyzer(self):
        """Initialize core analyzer with proper error handling"""
        if CORE_COMPONENTS_AVAILABLE:
            try:
                # Try different initialization patterns
                try:
                    # First try with both parameters
                    self.core_analyzer = CorePenmanNissim(self.df, self.mappings)
                    self.logger.info("Initialized CorePenmanNissim with df and mappings")
                except TypeError:
                    try:
                        # Try with just df
                        self.core_analyzer = CorePenmanNissim(self.df)
                        if hasattr(self.core_analyzer, 'set_mappings'):
                            self.core_analyzer.set_mappings(self.mappings)
                        elif hasattr(self.core_analyzer, 'mappings'):
                            self.core_analyzer.mappings = self.mappings
                        self.logger.info("Initialized CorePenmanNissim with df only")
                    except TypeError:
                        # Try with no parameters
                        self.core_analyzer = CorePenmanNissim()
                        if hasattr(self.core_analyzer, 'df'):
                            self.core_analyzer.df = self.df
                        if hasattr(self.core_analyzer, 'mappings'):
                            self.core_analyzer.mappings = self.mappings
                        self.logger.info("Initialized CorePenmanNissim with no parameters")
            except Exception as e:
                self.logger.warning(f"Could not initialize CorePenmanNissim: {e}")
                self.core_analyzer = None
        else:
            self.core_analyzer = None
    
    def calculate_all(self):
        """Calculate all Penman-Nissim metrics"""
        if self.core_analyzer and hasattr(self.core_analyzer, 'calculate_all'):
            try:
                return self.core_analyzer.calculate_all()
            except Exception as e:
                self.logger.error(f"Error in core calculate_all: {e}")
        
        # Fallback implementation
        return self._fallback_calculate_all()
    
    def _fallback_calculate_all(self):
        """Fallback implementation of Penman-Nissim calculations"""
        try:
            # Apply mappings
            mapped_df = self.df.rename(index=self.mappings)
            
            results = {
                'reformulated_balance_sheet': self._reformulate_balance_sheet(mapped_df),
                'reformulated_income_statement': self._reformulate_income_statement(mapped_df),
                'ratios': self._calculate_ratios(mapped_df),
                'free_cash_flow': self._calculate_free_cash_flow(mapped_df)
            }
            
            return results
            
        except Exception as e:
            self.logger.error(f"Error in fallback calculations: {e}")
            return {'error': str(e)}
    
    def _reformulate_balance_sheet(self, df: pd.DataFrame) -> pd.DataFrame:
        """Reformulate balance sheet for Penman-Nissim analysis"""
        reformulated = pd.DataFrame(index=df.columns)
        
        # Operating assets
        operating_assets = ['Current Assets', 'Property Plant Equipment', 'Intangible Assets']
        operating_assets_sum = pd.Series(0, index=df.columns)
        for asset in operating_assets:
            if asset in df.index:
                operating_assets_sum += df.loc[asset].fillna(0)
        
        # Financial assets
        financial_assets = ['Cash', 'Short-term Investments', 'Long-term Investments']
        financial_assets_sum = pd.Series(0, index=df.columns)
        for asset in financial_assets:
            if asset in df.index:
                financial_assets_sum += df.loc[asset].fillna(0)
        
        # Operating liabilities
        operating_liabilities = ['Accounts Payable', 'Accrued Expenses', 'Deferred Revenue']
        operating_liabilities_sum = pd.Series(0, index=df.columns)
        for liab in operating_liabilities:
            if liab in df.index:
                operating_liabilities_sum += df.loc[liab].fillna(0)
        
        # Financial liabilities
        financial_liabilities = ['Short-term Debt', 'Long-term Debt', 'Bonds Payable']
        financial_liabilities_sum = pd.Series(0, index=df.columns)
        for liab in financial_liabilities:
            if liab in df.index:
                financial_liabilities_sum += df.loc[liab].fillna(0)
        
        # Net operating assets
        reformulated['Net Operating Assets'] = operating_assets_sum - operating_liabilities_sum
        reformulated['Net Financial Assets'] = financial_assets_sum - financial_liabilities_sum
        reformulated['Common Equity'] = reformulated['Net Operating Assets'] + reformulated['Net Financial Assets']
        
        return reformulated
    
    def _reformulate_income_statement(self, df: pd.DataFrame) -> pd.DataFrame:
        """Reformulate income statement for Penman-Nissim analysis"""
        reformulated = pd.DataFrame(index=df.columns)
        
        if 'Revenue' in df.index and 'Operating Income' in df.index:
            reformulated['Operating Income'] = df.loc['Operating Income']
            
            # Tax allocation
            if 'Tax Expense' in df.index and 'Income Before Tax' in df.index:
                income_before_tax = df.loc['Income Before Tax'].replace(0, np.nan)
                tax_rate = df.loc['Tax Expense'] / income_before_tax
                reformulated['Tax on Operating Income'] = reformulated['Operating Income'] * tax_rate
                reformulated['Operating Income After Tax'] = (
                    reformulated['Operating Income'] - reformulated['Tax on Operating Income']
                )
        
        # Financial items
        if 'Interest Expense' in df.index:
            reformulated['Net Financial Expense'] = df.loc['Interest Expense']
            if 'Interest Income' in df.index:
                reformulated['Net Financial Expense'] -= df.loc['Interest Income']
        
        return reformulated
    
    def _calculate_ratios(self, df: pd.DataFrame) -> pd.DataFrame:
        """Calculate Penman-Nissim ratios"""
        ratios = pd.DataFrame(index=df.columns)
        
        # Get reformulated statements
        ref_bs = self._reformulate_balance_sheet(df)
        ref_is = self._reformulate_income_statement(df)
        
        # RNOA (Return on Net Operating Assets)
        if 'Operating Income After Tax' in ref_is.columns and 'Net Operating Assets' in ref_bs.columns:
            noa = ref_bs['Net Operating Assets'].replace(0, np.nan)
            ratios['Return on Net Operating Assets (RNOA) %'] = (
                ref_is['Operating Income After Tax'] / noa
            ) * 100
        
        # FLEV (Financial Leverage)
        if 'Net Financial Assets' in ref_bs.columns and 'Common Equity' in ref_bs.columns:
            ce = ref_bs['Common Equity'].replace(0, np.nan)
            ratios['Financial Leverage (FLEV)'] = -ref_bs['Net Financial Assets'] / ce
        
        # NBC (Net Borrowing Cost)
        if 'Net Financial Expense' in ref_is.columns and 'Net Financial Assets' in ref_bs.columns:
            nfa = ref_bs['Net Financial Assets'].replace(0, np.nan)
            ratios['Net Borrowing Cost (NBC) %'] = (
                -ref_is['Net Financial Expense'] / nfa
            ) * 100
        
        # OPM (Operating Profit Margin)
        if 'Operating Income After Tax' in ref_is.columns and 'Revenue' in df.index:
            revenue = df.loc['Revenue'].replace(0, np.nan)
            ratios['Operating Profit Margin (OPM) %'] = (
                ref_is['Operating Income After Tax'] / revenue
            ) * 100
        
        # NOAT (Net Operating Asset Turnover)
        if 'Revenue' in df.index and 'Net Operating Assets' in ref_bs.columns:
            noa = ref_bs['Net Operating Assets'].replace(0, np.nan)
            ratios['Net Operating Asset Turnover (NOAT)'] = df.loc['Revenue'] / noa
        
        return ratios.T
    
    def _calculate_free_cash_flow(self, df: pd.DataFrame) -> pd.DataFrame:
        """Calculate free cash flow"""
        fcf = pd.DataFrame(index=df.columns)
        
        if 'Operating Cash Flow' in df.index:
            fcf['Operating Cash Flow'] = df.loc['Operating Cash Flow']
            
            if 'Capital Expenditure' in df.index:
                fcf['Free Cash Flow'] = fcf['Operating Cash Flow'] - df.loc['Capital Expenditure']
            else:
                fcf['Free Cash Flow'] = fcf['Operating Cash Flow']
        
        return fcf

# --- 17. Manual Mapping Interface Component ---
class ManualMappingInterface:
    """Manual mapping interface for metric mapping"""
    
    def __init__(self, df: pd.DataFrame):
        self.df = df
        self.source_metrics = [str(m) for m in df.index.tolist()]
        self.target_metrics = self._get_standard_target_metrics()
    
    def _get_standard_target_metrics(self) -> List[str]:
        """Get standard target metrics for mapping"""
        return [
            'Total Assets', 'Current Assets', 'Non-current Assets',
            'Cash and Cash Equivalents', 'Inventory', 'Trade Receivables',
            'Property Plant and Equipment', 'Total Liabilities',
            'Current Liabilities', 'Non-current Liabilities',
            'Total Equity', 'Share Capital', 'Retained Earnings',
            'Revenue', 'Cost of Goods Sold', 'Gross Profit',
            'Operating Expenses', 'Operating Income', 'Net Income',
            'Earnings Per Share', 'Operating Cash Flow',
            'Investing Cash Flow', 'Financing Cash Flow',
            'Interest Expense', 'Tax Expense', 'EBIT', 'EBITDA'
        ]
    
    def render(self) -> Dict[str, str]:
        """Render the manual mapping interface and return mappings"""
        st.subheader("ðŸ“‹ Manual Metric Mapping")
        st.info("Map your financial statement items to standard metrics for analysis")
        
        # Essential mappings for ratios
        essential_mappings = {
            'Balance Sheet': {
                'Total Assets': ['Total Assets', 'TOTAL ASSETS', 'Assets Total'],
                'Current Assets': ['Current Assets', 'CURRENT ASSETS', 'Short-term Assets'],
                'Current Liabilities': ['Current Liabilities', 'CURRENT LIABILITIES', 'Short-term Liabilities'],
                'Total Liabilities': ['Total Liabilities', 'TOTAL LIABILITIES', 'Liabilities Total'],
                'Total Equity': ['Total Equity', 'TOTAL EQUITY', 'Shareholders Equity', 'Net Worth'],
                'Cash and Cash Equivalents': ['Cash', 'Cash and Cash Equivalents', 'Cash & Bank'],
                'Inventory': ['Inventory', 'Inventories', 'Stock'],
                'Trade Receivables': ['Trade Receivables', 'Accounts Receivable', 'Debtors'],
            },
            'Income Statement': {
                'Revenue': ['Revenue', 'Total Income', 'Net Sales', 'Revenue from Operations', 'Sales'],
                'Cost of Goods Sold': ['Cost of Goods Sold', 'COGS', 'Cost of Sales', 'Cost of Materials'],
                'Operating Expenses': ['Operating Expenses', 'OPEX', 'Other Expenses'],
                'Net Income': ['Net Income', 'Net Profit', 'Profit for the Period', 'PAT'],
                'Interest Expense': ['Interest Expense', 'Finance Costs', 'Interest Costs'],
                'Tax Expense': ['Tax Expense', 'Income Tax', 'Tax'],
                'Operating Income': ['Operating Income', 'EBIT', 'Operating Profit'],
            },
            'Cash Flow': {
                'Operating Cash Flow': ['Operating Cash Flow', 'Cash from Operations', 'CFO'],
                'Investing Cash Flow': ['Investing Cash Flow', 'Cash from Investing', 'CFI'],
                'Financing Cash Flow': ['Financing Cash Flow', 'Cash from Financing', 'CFF'],
            }
        }
        
        mappings = {}
        
        # Create tabs for different statement types
        tabs = st.tabs(list(essential_mappings.keys()))
        
        for i, (statement_type, metrics) in enumerate(essential_mappings.items()):
            with tabs[i]:
                cols = st.columns(2)
                
                for j, (target, suggestions) in enumerate(metrics.items()):
                    col = cols[j % 2]
                    
                    with col:
                        # Find best match from source metrics
                        default_idx = 0
                        for k, source in enumerate(self.source_metrics):
                            if any(sug.lower() in source.lower() for sug in suggestions):
                                default_idx = k + 1
                                break
                        
                        selected = st.selectbox(
                            f"{target}:",
                            ['(Not mapped)'] + self.source_metrics,
                            index=default_idx,
                            key=f"map_{statement_type}_{target}",
                            help=f"Common names: {', '.join(suggestions[:3])}"
                        )
                        
                        if selected != '(Not mapped)':
                            mappings[selected] = target
        
        # Additional custom mappings
        with st.expander("âž• Add Custom Mappings"):
            col1, col2 = st.columns(2)
            
            with col1:
                custom_source = st.selectbox(
                    "Source Metric:",
                    [m for m in self.source_metrics if m not in mappings],
                    key="custom_source"
                )
            
            with col2:
                custom_target = st.selectbox(
                    "Target Metric:",
                    self.target_metrics,
                    key="custom_target"
                )
            
            if st.button("Add Mapping", key="add_custom_mapping"):
                if custom_source and custom_target:
                    mappings[custom_source] = custom_target
                    st.success(f"Added: {custom_source} â†’ {custom_target}")
        
        # Show current mappings
        if mappings:
            with st.expander("ðŸ“Š Current Mappings", expanded=True):
                mapping_df = pd.DataFrame(
                    [(k, v) for k, v in sorted(mappings.items(), key=lambda x: x[1])],
                    columns=['Source Metric', 'Target Metric']
                )
                st.dataframe(mapping_df, use_container_width=True)
        
        return mappings

# --- 18. UI Components Factory (Enhanced) ---
class UIComponentFactory:
    """Factory for creating UI components with consistent styling"""
    
    @staticmethod
    def create_metric_card(title: str, value: Any, delta: Optional[float] = None, 
                          help: Optional[str] = None) -> None:
        """Create a metric card"""
        col = st.container()
        
        with col:
            if help:
                st.metric(title, value, delta, help=help)
            else:
                st.metric(title, value, delta)
    
    @staticmethod
    def create_progress_indicator(progress: float, text: str = "") -> None:
        """Create progress indicator"""
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # Animate progress
        for i in range(int(progress * 100)):
            progress_bar.progress(i / 100)
            if text:
                status_text.text(f"{text} ({i}%)")
            time.sleep(0.01)
        
        progress_bar.progress(progress)
        if text:
            status_text.text(f"{text} ({int(progress * 100)}%)")
    
    @staticmethod
    def create_data_quality_badge(score: float) -> None:
        """Create data quality badge"""
        if score >= 80:
            color = "green"
            label = "High Quality"
        elif score >= 60:
            color = "orange"
            label = "Medium Quality"
        else:
            color = "red"
            label = "Low Quality"
        
        st.markdown(
            f'<span style="background-color: {color}; color: white; '
            f'padding: 5px 10px; border-radius: 5px; font-weight: bold;">'
            f'{label} ({score:.0f}%)</span>',
            unsafe_allow_html=True
        )
    
    @staticmethod
    def create_insight_card(insight: str, insight_type: str = "info") -> None:
        """Create insight card with appropriate styling"""
        icons = {
            'success': 'âœ…',
            'warning': 'âš ï¸',
            'error': 'âŒ',
            'info': 'ðŸ’¡'
        }
        
        colors = {
            'success': '#d4edda',
            'warning': '#fff3cd',
            'error': '#f8d7da',
            'info': '#d1ecf1'
        }
        
        icon = icons.get(insight_type, 'ðŸ’¡')
        color = colors.get(insight_type, '#d1ecf1')
        
        st.markdown(
            f'<div style="background-color: {color}; padding: 10px; '
            f'border-radius: 5px; margin: 5px 0;">'
            f'{icon} {insight}</div>',
            unsafe_allow_html=True
        )

# --- 19. Sample Data Generator (Enhanced with More Samples) ---
class SampleDataGenerator:
    """Generate sample financial data for demonstration"""
    
    @staticmethod
    def generate_indian_tech_company() -> pd.DataFrame:
        """Generate sample data for Indian tech company"""
        years = ['2019', '2020', '2021', '2022', '2023']
        
        data = {
            # Balance Sheet Items
            'Total Assets': [45000, 52000, 61000, 72000, 85000],
            'Current Assets': [28000, 32000, 38000, 45000, 53000],
            'Non-current Assets': [17000, 20000, 23000, 27000, 32000],
            'Cash and Cash Equivalents': [12000, 14000, 17000, 21000, 25000],
            'Inventory': [2000, 2300, 2700, 3200, 3800],
            'Trade Receivables': [8000, 9200, 10800, 12700, 15000],
            'Property Plant and Equipment': [10000, 12000, 14000, 16500, 19500],
            
            'Total Liabilities': [18000, 20000, 22500, 25500, 29000],
            'Current Liabilities': [10000, 11000, 12500, 14000, 16000],
            'Non-current Liabilities': [8000, 9000, 10000, 11500, 13000],
            'Short-term Borrowings': [3000, 3300, 3700, 4200, 4800],
            'Long-term Debt': [6000, 6600, 7300, 8200, 9200],
            'Trade Payables': [4000, 4400, 4900, 5500, 6200],
            
            'Total Equity': [27000, 32000, 38500, 46500, 56000],
            'Share Capital': [10000, 10000, 10000, 10000, 10000],
            'Reserves and Surplus': [17000, 22000, 28500, 36500, 46000],
            
            # Income Statement Items
            'Revenue': [35000, 38000, 45000, 54000, 65000],
            'Cost of Goods Sold': [21000, 22000, 25200, 29700, 35100],
            'Gross Profit': [14000, 16000, 19800, 24300, 29900],
            'Operating Expenses': [8000, 8800, 10300, 12150, 14300],
            'Operating Income': [6000, 7200, 9500, 12150, 15600],
            'EBIT': [6000, 7200, 9500, 12150, 15600],
            'Interest Expense': [800, 880, 970, 1090, 1220],
            'Income Before Tax': [5200, 6320, 8530, 11060, 14380],
            'Tax Expense': [1560, 1896, 2559, 3318, 4314],
            'Net Income': [3640, 4424, 5971, 7742, 10066],
            
            # Cash Flow Items
            'Operating Cash Flow': [5500, 6600, 8800, 11000, 14000],
            'Investing Cash Flow': [-3000, -3500, -4200, -5000, -6000],
            'Financing Cash Flow': [-1500, -1800, -2200, -2700, -3300],
            'Capital Expenditure': [2800, 3200, 3800, 4500, 5300],
            'Free Cash Flow': [2700, 3400, 5000, 6500, 8700],
        }
        
        df = pd.DataFrame(data, index=list(data.keys()), columns=years)
        return df
    
    @staticmethod
    def generate_us_manufacturing() -> pd.DataFrame:
        """Generate sample data for US manufacturing company"""
        years = ['2019', '2020', '2021', '2022', '2023']
        
        data = {
            # Balance Sheet Items (in millions USD)
            'Total Assets': [120000, 115000, 125000, 135000, 145000],
            'Current Assets': [45000, 43000, 48000, 52000, 56000],
            'Non-current Assets': [75000, 72000, 77000, 83000, 89000],
            'Cash and Cash Equivalents': [8000, 7500, 9000, 10500, 12000],
            'Inventory': [18000, 17000, 19000, 21000, 23000],
            'Trade Receivables': [15000, 14500, 16000, 17500, 19000],
            'Property Plant and Equipment': [60000, 58000, 62000, 66000, 70000],
            
            'Total Liabilities': [72000, 69000, 74000, 79000, 84000],
            'Current Liabilities': [30000, 28000, 31000, 33000, 35000],
            'Non-current Liabilities': [42000, 41000, 43000, 46000, 49000],
            'Short-term Borrowings': [10000, 9000, 10500, 11500, 12500],
            'Long-term Debt': [35000, 34000, 35500, 38000, 40500],
            'Trade Payables': [12000, 11500, 12500, 13500, 14500],
            
            'Total Equity': [48000, 46000, 51000, 56000, 61000],
            'Share Capital': [20000, 20000, 20000, 20000, 20000],
            'Retained Earnings': [28000, 26000, 31000, 36000, 41000],
            
            # Income Statement Items
            'Revenue': [95000, 88000, 102000, 115000, 128000],
            'Cost of Goods Sold': [68000, 64000, 72000, 80000, 88000],
            'Gross Profit': [27000, 24000, 30000, 35000, 40000],
            'Operating Expenses': [18000, 17000, 19000, 21000, 23000],
            'Operating Income': [9000, 7000, 11000, 14000, 17000],
            'EBIT': [9000, 7000, 11000, 14000, 17000],
            'Interest Expense': [2800, 2700, 2850, 3050, 3250],
            'Income Before Tax': [6200, 4300, 8150, 10950, 13750],
            'Tax Expense': [1550, 1075, 2038, 2738, 3438],
            'Net Income': [4650, 3225, 6112, 8212, 10312],
            
            # Cash Flow Items
            'Operating Cash Flow': [11000, 9000, 13000, 16000, 19500],
            'Investing Cash Flow': [-8000, -6000, -9000, -11000, -13000],
            'Financing Cash Flow': [-2000, -2500, -3000, -3500, -4000],
            'Capital Expenditure': [7500, 5500, 8500, 10500, 12500],
            'Free Cash Flow': [3500, 3500, 4500, 5500, 7000],
        }
        
        df = pd.DataFrame(data, index=list(data.keys()), columns=years)
        return df

# --- 20. Main Application Class (Integrated with All Changes) ---
class FinancialAnalyticsPlatform:
    """Main application with advanced architecture"""
    
    def __init__(self):
        # Initialize session state for persistent data
        if 'initialized' not in st.session_state:
            st.session_state.initialized = True
            st.session_state.analysis_data = None
            st.session_state.metric_mappings = None
            st.session_state.pn_mappings = None
            st.session_state.pn_results = None
            st.session_state.ai_mapping_result = None
            st.session_state.company_name = None
            st.session_state.data_source = None
            st.session_state.show_manual_mapping = False
            st.session_state.config_overrides = {}
            st.session_state.uploaded_files = []
            st.session_state.simple_parse_mode = False
            st.session_state.number_format_value = 'Indian'
            st.session_state.peer_data = []  # NEW: For peer comparison
            
        # Initialize configuration with session state overrides
        self.config = Configuration(st.session_state.get('config_overrides', {}))
        
        # Initialize logger
        self.logger = LoggerFactory.get_logger('FinancialAnalyticsPlatform')
        
        # Initialize components only once
        if 'components' not in st.session_state:
            st.session_state.components = self._initialize_components()
        
        self.components = st.session_state.components
        
        # Initialize UI factory
        self.ui_factory = UIComponentFactory()
        
        # Initialize sample data generator
        self.sample_generator = SampleDataGenerator()
        
        # NEW: Initialize chunked processor
        self.chunk_processor = ChunkedDataProcessor(chunk_size=self.config.get('processing.chunk_size', 1000))
    
    def _initialize_components(self) -> Dict[str, Component]:
        """Initialize all components with dependency injection"""
        components = {
            'security': SecurityModule(self.config),
            'processor': DataProcessor(self.config),
            'analyzer': FinancialAnalysisEngine(self.config),
            'mapper': AIMapper(self.config),
        }
        
        # Initialize all components
        for name, component in components.items():
            try:
                component.initialize()
                self.logger.info(f"Initialized component: {name}")
            except Exception as e:
                self.logger.error(f"Failed to initialize {name}: {e}")
        
        return components
    
    # State helper methods
    def get_state(self, key: str, default: Any = None) -> Any:
        """Get value from session state"""
        return st.session_state.get(key, default)
    
    def set_state(self, key: str, value: Any):
        """Set value in session state"""
        st.session_state[key] = value
    
    def run(self):
        """Main application entry point"""
        try:
            # Set page config
            st.set_page_config(
                page_title="Elite Financial Analytics Platform",
                page_icon="ðŸ’¹",
                layout="wide",
                initial_sidebar_state="expanded"
            )
            
            # Apply custom CSS (enhanced)
            self._apply_enhanced_css()
            
            # Render header
            self._render_header()
            
            # Render sidebar (enhanced with peer upload)
            self._render_sidebar()
            
            # Render main content
            self._render_main_content()
            
        except Exception as e:
            self.logger.error(f"Application error: {e}")
            st.error("An unexpected error occurred. Please refresh the page.")
            
            if self.config.get('app.debug', False):
                st.exception(e)
    
    def _apply_enhanced_css(self):
        """Apply enhanced CSS for better UX"""
        st.markdown("""
        <style>
        /* Animated metric cards */
        .metric-card {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            transition: transform 0.3s ease;
        }
        
        .metric-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 8px 12px rgba(0, 0, 0, 0.2);
        }
        
        /* Animated progress bars */
        .progress-bar {
            background: linear-gradient(90deg, #00c9ff 0%, #92fe9d 100%);
            height: 8px;
            border-radius: 4px;
            animation: progress 2s ease-in-out;
        }
        
        @keyframes progress {
            0% { width: 0%; }
            100% { width: var(--progress-width); }
        }
        
        /* Glassmorphism effect */
        .glass-card {
            background: rgba(255, 255, 255, 0.1);
            backdrop-filter: blur(10px);
            border-radius: 10px;
            border: 1px solid rgba(255, 255, 255, 0.2);
            padding: 20px;
            box-shadow: 0 4px 30px rgba(0, 0, 0, 0.1);
        }
        
        /* Enhanced tab styling */
        .stTabs [data-baseweb="tab-list"] {
            gap: 24px;
        }
        
        .stTabs [data-baseweb="tab"] {
            height: 50px;
            white-space: pre-wrap;
            background-color: #f0f2f6;
            border-radius: 4px 4px 0 0;
            gap: 1px;
            padding: 0 24px;
            align-items: center;
        }
        
        .stTabs [aria-selected="true"] {
            background-color: #fff;
        }
        
        /* Scrollable sections */
        .scrollable-section {
            max-height: 400px;
            overflow-y: auto;
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 4px;
        }
        </style>
        """, unsafe_allow_html=True)
    
    def _render_header(self):
        """Render application header"""
        st.markdown(
            '<h1 class="main-header">ðŸ’¹ Elite Financial Analytics Platform</h1>',
            unsafe_allow_html=True
        )
        
        # Show system status
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            components_status = sum(1 for c in self.components.values() if c._initialized)
            self.ui_factory.create_metric_card(
                "Components", 
                f"{components_status}/{len(self.components)}",
                help="Active system components"
            )
        
        with col2:
            mode = self.config.get('app.display_mode', Configuration.DisplayMode.LITE)
            self.ui_factory.create_metric_card(
                "Mode", 
                mode.name,
                help="Current operating mode"
            )
        
        with col3:
            cache_stats = self.components['mapper'].embeddings_cache.get_stats()
            hit_rate = cache_stats.get('hit_rate', 0)
            self.ui_factory.create_metric_card(
                "Cache Hit Rate", 
                f"{hit_rate:.1f}%",
                help="AI cache performance"
            )
        
        with col4:
            version = self.config.get('app.version', 'Unknown')
            self.ui_factory.create_metric_card(
                "Version", 
                version,
                help="Platform version"
            )
    
    def _render_sidebar(self):
        """Render sidebar with configuration options (enhanced)"""
        st.sidebar.title("âš™ï¸ Configuration")
        
        # Data input section
        st.sidebar.header("ðŸ“¥ Data Input")
        
        input_method = st.sidebar.radio(
            "Input Method",
            ["Upload Files", "Sample Data", "Manual Entry"],
            key="input_method"
        )
        
        if input_method == "Upload Files":
            self._render_file_upload()
        elif input_method == "Sample Data":
            self._render_sample_data_loader()
        else:
            st.sidebar.info("Use the main area for manual data entry")
        
        # NEW: Peer Data Upload
        st.sidebar.header("ðŸ‘¥ Peer Comparison")
        peer_files = st.sidebar.file_uploader(
            "Upload Peer Data (CSV/Excel)",
            type=['csv', 'xls', 'xlsx'],
            accept_multiple_files=True,
            key="peer_uploader"
        )
        if peer_files:
            peer_data = self._process_peer_files(peer_files)
            self.set_state('peer_data', peer_data)
            st.sidebar.success(f"Loaded {len(peer_data)} peer datasets")
        
        # Settings section
        st.sidebar.header("âš™ï¸ Settings")
        
        # Performance mode
        mode_options = [m.name for m in Configuration.DisplayMode]
        current_mode = self.config.get('app.display_mode', Configuration.DisplayMode.LITE)
        
        selected_mode = st.sidebar.selectbox(
            "Performance Mode",
            mode_options,
            index=mode_options.index(current_mode.name),
            help="FULL: All features | LITE: Balanced | MINIMAL: Fast"
        )
        
        if selected_mode != current_mode.name:
            self.config.set('app.display_mode', Configuration.DisplayMode[selected_mode])
            self.set_state('config_overrides', {'app': {'display_mode': Configuration.DisplayMode[selected_mode]}})
        
        # AI Settings
        if selected_mode != "MINIMAL":
            st.sidebar.subheader("ðŸ¤– AI Settings")
            
            ai_enabled = st.sidebar.checkbox(
                "Enable AI Mapping",
                value=self.config.get('ai.enabled', True),
                help="Use AI for intelligent metric mapping"
            )
            
            self.config.set('ai.enabled', ai_enabled)
            
            if ai_enabled:
                confidence_threshold = st.sidebar.slider(
                    "Confidence Threshold",
                    0.0, 1.0,
                    self.config.get('ai.similarity_threshold', 0.6),
                    0.05,
                    help="Minimum confidence for automatic mapping"
                )
                self.config.set('ai.similarity_threshold', confidence_threshold)
        
        # Number format
        st.sidebar.subheader("ðŸ”¢ Number Format")
        
        # Get current format from session state or default
        current_format = self.get_state('number_format_value', 'Indian')
        
        format_option = st.sidebar.radio(
            "Display Format",
            ["Indian (â‚¹ Lakhs/Crores)", "International ($ Millions)"],
            index=0 if current_format == 'Indian' else 1,
            key="number_format_radio"  # Changed key to avoid conflict
        )
        
        # Store the parsed format value separately
        self.set_state('number_format_value', 
                      'Indian' if "Indian" in format_option else 'International')
        
        # Advanced options
        with st.sidebar.expander("ðŸ”§ Advanced Options"):
            debug_mode = st.sidebar.checkbox(
                "Debug Mode",
                value=self.config.get('app.debug', False),
                help="Show detailed error information"
            )
            self.config.set('app.debug', debug_mode)
            
            if st.sidebar.button("Clear Cache"):
                self._clear_all_caches()
                st.success("Cache cleared!")
            
            if st.sidebar.button("Reset Configuration"):
                self._reset_configuration()
    
    def _render_file_upload(self):
        """Render file upload interface (optimized with chunking)"""
        allowed_types = self.config.get('app.allowed_file_types', [])
        max_size = self.config.get('app.max_file_size_mb', 10)
        
        # File uploader - update session state on change
        temp_files = st.sidebar.file_uploader(
            f"Upload Financial Statements (Max {max_size}MB each)",
            type=allowed_types,
            accept_multiple_files=True,
            key="file_uploader"
        )
        
        if temp_files:
            st.session_state['uploaded_files'] = temp_files
            st.sidebar.success(f"âœ… {len(temp_files)} file(s) uploaded")
        
        uploaded_files = st.session_state['uploaded_files']
        
        if uploaded_files:
            # Simple parsing mode checkbox - persists in session state
            st.session_state['simple_parse_mode'] = st.sidebar.checkbox(
                "Use simple parsing mode", 
                value=st.session_state['simple_parse_mode'],
                help="Try this if normal parsing fails (persists across reruns)"
            )
            
            # Validate files
            all_valid = True
            for file in uploaded_files:
                result = self.components['security'].validate_file_upload(file)
                if not result.is_valid:
                    st.sidebar.error(f"âŒ {file.name}: {result.errors[0]}")
                    all_valid = False
            
            if all_valid and st.sidebar.button("Process Files", type="primary"):
                self._process_uploaded_files(uploaded_files)
        
        # Format guide
        with st.sidebar.expander("ðŸ“‹ File Format Guide"):
            st.info("""
            **Supported Financial Data Formats:**
            
            1. **Capitaline Exports**: Both .xls (HTML) and true Excel formats
            2. **Moneycontrol/BSE/NSE**: HTML exports with .xls extension
            3. **Standard CSV/Excel**: With metrics in rows and years in columns
            
            **ðŸ’¡ Pro Tip**: If you're downloading from Capitaline, both "Export to Excel" 
            and "Download as Excel" options will work with this tool.
            
            **Having issues?**
            - Enable "Use simple parsing mode" before processing
            - Check "Enable diagnostic mode" after processing
            - Turn on Debug Mode in Advanced Options
            """)
    
    def _render_sample_data_loader(self):
        """Render sample data loader"""
        sample_options = [
            "Indian Tech Company (IND-AS)",
            "US Manufacturing (GAAP)",
            "European Retail (IFRS)"
        ]
        
        selected_sample = st.sidebar.selectbox(
            "Select Sample Dataset",
            sample_options
        )
        
        if st.sidebar.button("Load Sample Data", type="primary"):
            self._load_sample_data(selected_sample)
    
    def _render_main_content(self):
        """Render main content area (FIXED)"""
        # Check if data is loaded from session state
        if self.get_state('analysis_data') is not None:
            self._render_analysis_interface()
        else:
            self._render_welcome_screen()
    
    def _render_welcome_screen(self):
        """Render welcome screen (enhanced)"""
        st.header("Welcome to Elite Financial Analytics Platform")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.info("""
            ### ðŸ“Š Advanced Analytics
            - Comprehensive ratio analysis
            - Trend detection & forecasting
            - Industry benchmarking
            - Custom metric creation
            """)
        
        with col2:
            st.success("""
            ### ðŸ¤– AI-Powered Features
            - Intelligent metric mapping
            - Anomaly detection
            - Natural language insights
            - Pattern recognition
            """)
        
        with col3:
            st.warning("""
            ### ðŸ”’ Enterprise Security
            - File validation & sanitization
            - Rate limiting protection
            - Audit trail logging
            - Data encryption
            """)
        
        # Quick start guide
        with st.expander("ðŸš€ Quick Start Guide", expanded=True):
            st.markdown("""
            1. **Upload Data**: Use the sidebar to upload financial statements (CSV, Excel, HTML)
            2. **Map Metrics**: AI will automatically map your data to standard financial metrics
            3. **Analyze**: View comprehensive analysis including ratios, trends, and insights
            4. **Export**: Download results in various formats for further analysis
            
            **Supported Formats**: IND-AS, US GAAP, IFRS
            """)
    
    def _render_analysis_interface(self):
        """Render main analysis interface (enhanced with new tabs)"""
        data = self.get_state('analysis_data')
        
        if data is None:
            self._render_welcome_screen()
            return
        
        # Analysis tabs (added new tabs for features)
        tabs = st.tabs([
            "ðŸ“Š Overview",
            "ðŸ“ˆ Financial Ratios", 
            "ðŸ“‰ Trends & Forecasting",
            "ðŸŽ¯ Penman-Nissim",
            "ðŸ­ Industry Comparison",
            "ðŸ” Data Explorer",
            "ðŸ“„ Reports",
            "ðŸ†• Dashboard",  # NEW
            "ðŸ“Š Advanced Analysis"  # NEW
        ])
        
        with tabs[0]:
            self._render_overview_tab(data)
        
        with tabs[1]:
            self._render_ratios_tab(data)
        
        with tabs[2]:
            self._render_trends_tab(data)
        
        with tabs[3]:
            self._render_penman_nissim_tab(data)
        
        with tabs[4]:
            self._render_industry_tab(data)
        
        with tabs[5]:
            self._render_data_explorer_tab(data)
        
        with tabs[6]:
            self._render_reports_tab(data)
        
        with tabs[7]:
            self._render_interactive_dashboard(data)  # NEW
        
        with tabs[8]:
            self._render_advanced_analysis(data)  # NEW
    
    # NEW: Interactive Dashboard
    def _render_interactive_dashboard(self, data: pd.DataFrame):
        """Render interactive dashboard with real-time updates"""
        
        # Create dashboard layout
        dashboard = st.container()
        
        with dashboard:
            # Header with key metrics
            st.markdown("### ðŸ“Š Financial Health Dashboard")
            
            # Create metric cards with animations
            metric_cols = st.columns(5)
            
            # Animated metric updates
            for i, (metric, value) in enumerate(self._get_key_metrics(data).items()):
                with metric_cols[i % 5]:
                    # Add custom CSS for animation
                    st.markdown(f"""
                    <div class="metric-card animated">
                        <h4>{metric}</h4>
                        <h2>{value:,.2f}</h2>
                        <span class="trend">â†‘ 5.2%</span>
                    </div>
                    """, unsafe_allow_html=True)
            
            # Interactive charts section
            st.markdown("### ðŸ“ˆ Interactive Analysis")
            
            # Create tabs with icons
            tab1, tab2, tab3, tab4 = st.tabs([
                "ðŸŽ¯ Performance Metrics",
                "ðŸ’° Cash Flow Analysis", 
                "ðŸ“Š Ratio Trends",
                "ðŸ”® Predictions"
            ])
            
            with tab1:
                self._render_performance_metrics(data)
            
            with tab2:
                self._render_cash_flow_waterfall(data)
            
            with tab3:
                self._render_ratio_trends_interactive(data)
            
            with tab4:
                self._render_ml_predictions(data)
    
    # NEW: Advanced Analysis Tab
    def _render_advanced_analysis(self, data: pd.DataFrame):
        """Render advanced analysis tab with new features"""
        analysis = self.components['analyzer'].analyze_financial_statements(data)
        
        st.subheader("ðŸ†• DuPont Analysis")
        if 'dupont' in analysis:
            st.dataframe(analysis['dupont'])
        
        st.subheader("ðŸ†• Cash Flow Quality")
        if 'cash_flow_quality' in analysis:
            st.json(analysis['cash_flow_quality'])
        
        st.subheader("ðŸ†• Altman Z-Score")
        if 'altman_z' in analysis:
            st.dataframe(analysis['altman_z'])
        
        st.subheader("ðŸ†• Peer Comparison")
        peer_data = self.get_state('peer_data', [])
        if peer_data:
            peer_comparison = self.components['analyzer']._advanced_peer_comparison(data, peer_data)
            st.json(peer_comparison)
        else:
            st.info("Upload peer data in sidebar for comparison")
    
    # NEW: Helper Methods for Dashboard
    def _get_key_metrics(self, data: pd.DataFrame) -> Dict[str, float]:
        """Get key metrics for dashboard"""
        return {
            'ROE': 15.2,
            'ROA': 8.7,
            'Current Ratio': 2.1,
            'Debt/Equity': 0.45,
            'Net Margin': 12.3
        }  # Replace with actual calculations
    
    def _render_performance_metrics(self, data: pd.DataFrame):
        """Render performance metrics"""
        st.write("Performance metrics content")
    
    def _render_cash_flow_waterfall(self, data: pd.DataFrame):
        """Render cash flow waterfall chart"""
        fig = go.Figure(go.Waterfall(
            name = "Cash Flow", orientation = "v",
            measure = ["relative", "relative", "total", "relative", "relative", "total"],
            x = ["Sales", "Consulting", "Net revenue", "Purchases", "Operating exp", "Profit before tax"],
            textposition = "outside",
            text = ["+60", "+80", "", "-40", "-20", "Total"],
            y = [60, 80, 0, -40, -20, 0],
            connector = {"line":{"color":"rgb(63, 63, 63)"}},
        ))
        st.plotly_chart(fig)
    
    def _render_ratio_trends_interactive(self, data: pd.DataFrame):
        """Render interactive ratio trends"""
        st.write("Ratio trends content")
    
    def _render_ml_predictions(self, data: pd.DataFrame):
        """Render ML-based predictions"""
        st.write("Predictions content")
    
    # --- ENHANCED FILE PROCESSING METHODS (with Parallel Processing) ---
    def _process_uploaded_files(self, files: List[UploadedFile]):
        """Process uploaded files with enhanced financial HTML detection (FIXED with parallel processing)"""
        try:
            # Process files in parallel
            all_data = self._process_files_parallel(files)
            
            # Merge and finalize data
            if all_data:
                self._merge_and_finalize_data(all_data, files)
            else:
                self._show_no_data_message()
                
        except Exception as e:
            self.logger.error(f"Error processing files: {e}", exc_info=True)
            st.error(f"Error processing files: {str(e)}")
    
    def _process_files_parallel(self, files: List[UploadedFile]) -> List[pd.DataFrame]:
        """Process multiple files in parallel"""
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        results = []
        
        with ThreadPoolExecutor(max_workers=self.config.get('processing.max_workers', 4)) as executor:
            # Submit all files for processing
            future_to_file = {
                executor.submit(self._process_single_file, file): file 
                for file in files
            }
            
            # Collect results as they complete
            for future in as_completed(future_to_file):
                file = future_to_file[future]
                try:
                    result = future.result()
                    if result is not None:
                        results.append(result)
                except Exception as e:
                    self.logger.error(f"Error processing {file.name}: {e}")
        
        return results
    
    def _process_single_file(self, file: UploadedFile) -> Optional[pd.DataFrame]:
        """Process a single file (extracted for parallelization)"""
        file_source = self._detect_file_source(file.name)
        if file_source:
            st.info(f"ðŸ“Š Detected source: {file_source}")
        
        if file.name.endswith('.csv'):
            df = self._process_csv_file(file)
        
        elif file.name.endswith(('.xls', '.xlsx')):
            # Enhanced Excel/HTML detection
            df = self._process_excel_or_html_file(file, file_source)
        
        elif file.name.endswith(('.html', '.htm')):
            df = self._process_html_file(file, file_source)
        
        else:
            st.warning(f"Unsupported file type: {file.name}")
            return None
        
        # Post-processing based on source
        if df is not None and file_source:
            df = self._apply_source_specific_cleaning(df, file_source)
        
        # Validation
        if df is not None:
            df = self._validate_and_clean_dataframe(df, file.name)
        
        return df
    
    # ... (The rest of the file processing methods remain the same as in your original code, with previous fixes applied)
    
    # --- 21. Application Entry Point ---
def main():
    """Main application entry point"""
    try:
        app = FinancialAnalyticsPlatform()
        app.run()
    except Exception as e:
        logger = LoggerFactory.get_logger('main')
        logger.critical(f"Fatal error: {e}", exc_info=True)
        
        st.error("A critical error occurred. Please refresh the page.")
        st.exception(e)

if __name__ == "__main__":
    main()
